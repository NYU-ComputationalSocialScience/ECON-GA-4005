{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e457a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Training Tips\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow\n",
    "- SGD\n",
    "- MLP\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the arguments to the keras `.fit` method\n",
    "- Be able to use tensorboard to track training progress\n",
    "- Be able to save a trained model and load it from disk for prediction\n",
    "- Introduce concept of transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d67410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b440c62",
   "metadata": {},
   "source": [
    "## Practical Keras\n",
    "\n",
    "- We've done some heavy lifting on math side\n",
    "- Now let's add in a touch of practicality and go over some best practices and features of using keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ae601",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "- The exact problem we are solving isn't extremely relevant here as our focus is on the techniques\n",
    "- That being said, we may as well have fun while we are at it!\n",
    "- This dataset is called \"Fashion MNIST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ae967",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fashion MNIST\n",
    "\n",
    "- Once MNIST was too easy for new ML models and algorithms, the researchers in the field created a new challenge: [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "- Instead of pictures of hand written digits, the Fashion MNIST dataset contains low-resolution images of articles of clothing\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9fc4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "26435584/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n",
      "Number of original training examples: 60000\n",
      "Number of original test examples: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(x_train[i, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f1c07",
   "metadata": {},
   "source": [
    "- Let's also define a helper function that will let us quickly define a new model so we can experiment later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7118b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # 1. define model\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # 2 compile model (choose optimizer and loss)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11eb8c9",
   "metadata": {},
   "source": [
    "## Arguments to`.fit`\n",
    "\n",
    "- Keras is a very powerful and flexible framework\n",
    "- This is necessary to handle the breadth of ML tasks the modern researcher has to perform\n",
    "- A lot of the flexibility is in the fit method\n",
    "    - `batch_size`\n",
    "    - `class_weight` and `sample_weight` for imbalanced datasets\n",
    "    - `initial_epoch` to continue training\n",
    "    - `validation_split` if we want keras to split for us\n",
    "    \n",
    "Reference: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fitReference: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e294795",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "- On large problems (either in number of observations or size of network), training can take a very long time\n",
    "- Sometimes it is helpful to be able to monitor the progress of training\n",
    "- Tensorflow has a built-in feature called tensorboard that allows you to see a live dashboard of training as it is happening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5647a8b",
   "metadata": {},
   "source": [
    "### Tensorboard with Keras\n",
    "\n",
    "- When using keras, it is very easy to integrate tensorboard\n",
    "- There are three main concepts:\n",
    "    1. Choose a directory for keras to write \"logs\"\n",
    "    2. Set up the tensorboard callback for the `.fit` method\n",
    "    3. Start tensorboard and point it at logs directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25b21a",
   "metadata": {},
   "source": [
    "We'll start the training below and load up tensorboard at the same time\n",
    "\n",
    "> Note: there is much more you can do with tensorboard... we are only covering the basics. Check out their [docs](https://www.tensorflow.org/tensorboard) for more ideas and details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3035728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.8207 - val_loss: 0.4375 - val_sparse_categorical_accuracy: 0.8460\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3785 - sparse_categorical_accuracy: 0.8652 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8641\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8773 - val_loss: 0.3898 - val_sparse_categorical_accuracy: 0.8683\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3281 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.4148 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3157 - sparse_categorical_accuracy: 0.8889 - val_loss: 0.3745 - val_sparse_categorical_accuracy: 0.8737\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3079 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.4556 - val_sparse_categorical_accuracy: 0.8515\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3001 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.3853 - val_sparse_categorical_accuracy: 0.8765\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2929 - sparse_categorical_accuracy: 0.8979 - val_loss: 0.3993 - val_sparse_categorical_accuracy: 0.8752\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2887 - sparse_categorical_accuracy: 0.9011 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.4252 - val_sparse_categorical_accuracy: 0.8687\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2733 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4147 - val_sparse_categorical_accuracy: 0.8747\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2688 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.4708 - val_sparse_categorical_accuracy: 0.8639\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2639 - sparse_categorical_accuracy: 0.9105 - val_loss: 0.4092 - val_sparse_categorical_accuracy: 0.8807\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2595 - sparse_categorical_accuracy: 0.9127 - val_loss: 0.4623 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2594 - sparse_categorical_accuracy: 0.9135 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.8746\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2509 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.4874 - val_sparse_categorical_accuracy: 0.8688\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2497 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.4631 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2447 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.8778\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2426 - sparse_categorical_accuracy: 0.9193 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8765\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2369 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.4727 - val_sparse_categorical_accuracy: 0.8807\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2346 - sparse_categorical_accuracy: 0.9224 - val_loss: 0.5371 - val_sparse_categorical_accuracy: 0.8778\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2314 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.4897 - val_sparse_categorical_accuracy: 0.8778\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2293 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.5150 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2265 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.4991 - val_sparse_categorical_accuracy: 0.8848\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2227 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.5695 - val_sparse_categorical_accuracy: 0.8738\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2184 - sparse_categorical_accuracy: 0.9292 - val_loss: 0.5648 - val_sparse_categorical_accuracy: 0.8725\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2150 - sparse_categorical_accuracy: 0.9296 - val_loss: 0.5735 - val_sparse_categorical_accuracy: 0.8767\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2159 - sparse_categorical_accuracy: 0.9317 - val_loss: 0.5871 - val_sparse_categorical_accuracy: 0.8756\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2117 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.5661 - val_sparse_categorical_accuracy: 0.8835\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2105 - sparse_categorical_accuracy: 0.9327 - val_loss: 0.6094 - val_sparse_categorical_accuracy: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff130ac4ee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1, choose logs directory\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H_%M\")\n",
    "log_dir = \"fashion_mnist_logs/fit/{}\".format(timestamp)\n",
    "\n",
    "# step 2. set up callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# step 3. fit model\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=30, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3c9bc",
   "metadata": {},
   "source": [
    "## Using our models \n",
    "\n",
    "- Now that we have fit our model, let's use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be9843",
   "metadata": {},
   "source": [
    "### Prediction (inference)\n",
    "\n",
    "- We can predict the label for new images using the `.predict` method\n",
    "- Using a trained model to make a prediction is called prediction, or sometimes by the deep learning community \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eddb186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0723240e-13, 1.0000000e+00, 9.4608033e-24, 1.0103242e-11,\n",
       "        1.9663989e-24, 1.0154721e-31, 2.7607355e-19, 0.0000000e+00,\n",
       "        2.0640438e-29, 3.7758777e-29]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test[3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13988a",
   "metadata": {},
   "source": [
    "- In this case the model seems very confident that the correct label is `1`\n",
    "- From the table above, the label `1` corresponds to \"Trouser\"\n",
    "- Let's see what the image looked like and what the true label is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15d0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[3])\n",
    "print(y_test[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dddf3",
   "metadata": {},
   "source": [
    "- Sure enough -- looks like pants and that's what the target was"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2286b11",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "- When using a machine learning in practice, we often need to train it in one python Session and use it somewhere else\n",
    "- This will require that we can effectively save and load a model to a hard disk\n",
    "- The `model.save` function will do just this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1273d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_backprop.ipynb            \u001b[34mfashion_mnist_logs\u001b[m\u001b[m\n",
      "02_training_with_keras.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874e16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_backprop.ipynb            \u001b[34mfashion_mnist_logs\u001b[m\u001b[m\n",
      "02_training_with_keras.ipynb fashion_mnist_model.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"fashion_mnist_model.h5\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadf60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"fashion_mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bc4d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0723240e-13, 1.0000000e+00, 9.4608033e-24, 1.0103242e-11,\n",
       "        1.9663989e-24, 1.0154721e-31, 2.7607355e-19, 0.0000000e+00,\n",
       "        2.0640438e-29, 3.7758777e-29]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(x_test[3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff4adf",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "- There are a few sub-problems in ML that are very well studied\n",
    "- You might even say they are mostly \"solved\"\n",
    "- One of this is image recognition -- we'll focus on this subproblem for the next few minutes\n",
    "- Question: can we leverage models trained on one dataset for prediction on another?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1217dd",
   "metadata": {},
   "source": [
    "### Network Intuitions\n",
    "\n",
    "- Because of the nested functions and many parameters, it is difficult to tell exactly why a neural network does what it does\n",
    "- It is somewhat of a \"black box\"\n",
    "- However, some work has been done to support a hypothesis that a network works as follows:\n",
    "    - Early layers in the network learn basic features: e.g. how to recognize a line, how to find the color red\n",
    "    - Subsequent layers know how to combine multiple basic features: a red line, or two lines that intersect at a right angle\n",
    "    - Later layers in the network combine the more advanced features into sophisticated representations: a long straight line with a red-eight sided polygon on top of it (a US stop sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee085c0e",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "- Because early layers find general features that appear in most images, the weights/biases they learn can be useful across problem domains:\n",
    "    - Dogs vs cats\n",
    "    - Digits\n",
    "    - Find your friends in a photo\n",
    "- Later layers use these generally applicable learned representations to tailor predictions to a task\n",
    "- This is the idea behind transfer learning: use an expertly crafted and trained neural network for your own task (even if it was trained for a different task of the same type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc82239",
   "metadata": {},
   "source": [
    "### Example: MobileNet\n",
    "\n",
    "- Typically, models that achieve state of the art performance are given names\n",
    "- One such model is called `MobileNet`\n",
    "- This model is famous because it has excellent performance, but is relatively small (compared to other modern neural networks) and thus applicable for mobile devices\n",
    "- Let's apply `MobileNet` on our Fashion MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db063930",
   "metadata": {},
   "source": [
    "#### Transfer learning strategy\n",
    "\n",
    "1. Define the shape of our data (28, 28, 1) -- necessary b/c MobileNet was trained images of a different size\n",
    "2. Load model, without final layer for predictions. Freeze all weights\n",
    "3. Add a `Dense(10)` layer to end of network for our problem\n",
    "4. Train our layer\n",
    "\n",
    "> Note: don't focus on code here -- just focus on the fact that this is possible. It is pretty incredible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477a9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 1s 0us/step\n",
      "9420800/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# step 1. define shapes\n",
    "input_image = keras.layers.Input(shape=(28, 28, 3))\n",
    "resized_image = keras.layers.Lambda(lambda image: keras.backend.resize_images(x=image, height_factor=2, width_factor=2, data_format='channels_last'))(input_image)\n",
    "\n",
    "# step 2. load model. set weights to fixed\n",
    "base_model = keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False, input_tensor=resized_image)\n",
    "\n",
    "# don't change parameters of the MobileNet model\n",
    "base_model.trainable = False\n",
    "\n",
    "# step 3. add some layers\n",
    "x = base_model.output\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# build model\n",
    "transfer_model = keras.models.Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88747ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 218s 113ms/step - loss: 0.5604 - sparse_categorical_accuracy: 0.8110 - val_loss: 0.4467 - val_sparse_categorical_accuracy: 0.8495\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 172s 92ms/step - loss: 0.4662 - sparse_categorical_accuracy: 0.8475 - val_loss: 0.4607 - val_sparse_categorical_accuracy: 0.8548\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 182s 97ms/step - loss: 0.4685 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.4510 - val_sparse_categorical_accuracy: 0.8583\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 256s 136ms/step - loss: 0.4689 - sparse_categorical_accuracy: 0.8540 - val_loss: 0.4585 - val_sparse_categorical_accuracy: 0.8605\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 181s 96ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.4893 - val_sparse_categorical_accuracy: 0.8527\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.4795 - sparse_categorical_accuracy: 0.8550 - val_loss: 0.4778 - val_sparse_categorical_accuracy: 0.8638\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 206s 110ms/step - loss: 0.4825 - sparse_categorical_accuracy: 0.8561 - val_loss: 0.4838 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.4866 - sparse_categorical_accuracy: 0.8564 - val_loss: 0.4961 - val_sparse_categorical_accuracy: 0.8607\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 183s 98ms/step - loss: 0.4893 - sparse_categorical_accuracy: 0.8581 - val_loss: 0.5060 - val_sparse_categorical_accuracy: 0.8585\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 214s 114ms/step - loss: 0.5001 - sparse_categorical_accuracy: 0.8568 - val_loss: 0.5046 - val_sparse_categorical_accuracy: 0.8632\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.5014 - sparse_categorical_accuracy: 0.8561 - val_loss: 0.5185 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5075 - sparse_categorical_accuracy: 0.8569 - val_loss: 0.5211 - val_sparse_categorical_accuracy: 0.8589\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 207s 110ms/step - loss: 0.5105 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.5322 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 0.5097 - sparse_categorical_accuracy: 0.8583 - val_loss: 0.5380 - val_sparse_categorical_accuracy: 0.8624\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 0.5165 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5540 - val_sparse_categorical_accuracy: 0.8573\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 0.5194 - sparse_categorical_accuracy: 0.8572 - val_loss: 0.5471 - val_sparse_categorical_accuracy: 0.8619\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 185s 98ms/step - loss: 0.5271 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.5521 - val_sparse_categorical_accuracy: 0.8615\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 213s 113ms/step - loss: 0.5236 - sparse_categorical_accuracy: 0.8578 - val_loss: 0.5646 - val_sparse_categorical_accuracy: 0.8568\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5319 - sparse_categorical_accuracy: 0.8571 - val_loss: 0.5644 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 262s 140ms/step - loss: 0.5369 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.5846 - val_sparse_categorical_accuracy: 0.8555\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 215s 115ms/step - loss: 0.5396 - sparse_categorical_accuracy: 0.8575 - val_loss: 0.5830 - val_sparse_categorical_accuracy: 0.8574\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 185s 98ms/step - loss: 0.5431 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.5802 - val_sparse_categorical_accuracy: 0.8602\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.8572 - val_loss: 0.6021 - val_sparse_categorical_accuracy: 0.8532\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5433 - sparse_categorical_accuracy: 0.8571 - val_loss: 0.5816 - val_sparse_categorical_accuracy: 0.8621\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 213s 114ms/step - loss: 0.5494 - sparse_categorical_accuracy: 0.8579 - val_loss: 0.6094 - val_sparse_categorical_accuracy: 0.8513\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 211s 112ms/step - loss: 0.5588 - sparse_categorical_accuracy: 0.8585 - val_loss: 0.5866 - val_sparse_categorical_accuracy: 0.8606\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 240s 128ms/step - loss: 0.5577 - sparse_categorical_accuracy: 0.8599 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.8602\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 300s 160ms/step - loss: 0.5538 - sparse_categorical_accuracy: 0.8589 - val_loss: 0.6203 - val_sparse_categorical_accuracy: 0.8598\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 334s 178ms/step - loss: 0.5556 - sparse_categorical_accuracy: 0.8587 - val_loss: 0.6075 - val_sparse_categorical_accuracy: 0.8593\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 300s 160ms/step - loss: 0.5676 - sparse_categorical_accuracy: 0.8579 - val_loss: 0.6114 - val_sparse_categorical_accuracy: 0.8578\n"
     ]
    }
   ],
   "source": [
    "# step 4. fit our layers\n",
    "transfer_model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "log_dir = \"fashion_mnist_logs/mobilenet/fit/{}\".format(timestamp)\n",
    "\n",
    "# step 2. set up callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "transfer_history = transfer_model.fit(\n",
    "    np.repeat(x_train, 3, -1), y_train, \n",
    "    epochs=30, \n",
    "    validation_data=(np.repeat(x_test, 3, -1), y_test),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8e8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31582b9feba862c420bc95ad7fac43fb721c474490d1710b4e50ac63470f9531"
  },
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
