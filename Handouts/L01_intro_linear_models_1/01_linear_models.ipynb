{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Models I\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Calculus\n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand landscape of ML problems\n",
    "- Know difference between regression and classification settings\n",
    "- Be able to apply linear regression for regression and logisitc regression for classification\n",
    "    - Using `statsmodels` and `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML Paradigms\n",
    "\n",
    "ML problems can be grouped into three broad categories\n",
    "\n",
    "The categorization of a problem depends on the type of task to be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "\n",
    "\n",
    "-   Learn to map features to targets (labels)\n",
    "-   Two main subtasks:\n",
    "    1.  Regression: targets are continuous $\\mathbb{Y} \\subseteq \\mathbb{R}^N$\n",
    "    2.  Classification: targets in discrete space $\\mathbb{Y} \\subseteq \\mathbb{N}^N$\n",
    "    \n",
    "    \n",
    "> More detail to come today..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "\n",
    "\n",
    "-   Discover structure without labels\n",
    "-   Many types of subtasks, including:\n",
    "    -   Clustering\n",
    "    -   Dimensionality reduction\n",
    "    -   Compression\n",
    "    \n",
    "> Not for today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "\n",
    "\n",
    "-   Main problem: agent (algorithm) learns to behave optimally through trial\n",
    "    and error\n",
    "-   Agent observes state $s \\in \\mathbb{S}$, takes an action $a \\in \\mathbb{A}(s)$, observes a reward $r \\in \\mathbb{R}$ and new state $s'\\in\\mathbb{S}$\n",
    "-   Goal is to maximize (discounted) sum of all rewards in potentially\n",
    "    infinite episode\n",
    "\n",
    "> Also not for today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "-   One of the most common ML tasks\n",
    "-   Based on available data, learn relationships between variables\n",
    "-   Alternatively: learn a mapping from an input (feature) space to an\n",
    "    output space\n",
    "\n",
    "![Simple Linear Regression](https://css-materials.s3.amazonaws.com/ML/linear_models_1/supervised_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some Examples\n",
    "\n",
    "-   Predicting gross income of yet-to-be-released movie\n",
    "-   Real estate pricing prediction\n",
    "-   Any time series forecasting problem\n",
    "-   Predicting wage based on demographic and educational features\n",
    "-   Identifying faces in a photo\n",
    "-   Predicting likelihood of default on a loan based on lender characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning: Data\n",
    "\n",
    "- Features or inputs: $\\mathbb{X} \\subseteq \\mathbb{R}^N$\n",
    "    - Might require transformations\n",
    "    - Images are matrix of RGB triplets\n",
    "    - Text can be tokenized where each word is assigned an integer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Output space $\\mathbb{Y} \\subseteq \\mathbb{R}^M$ (regression) or $\\mathbb{Y} \\subseteq \\mathbb{N}^M$ (classification)\n",
    "    - Classification often requires transformation:\n",
    "        - Discrete categories (A, B, C, D, F) into numbers (1, 2, 3, 4, 5)\n",
    "        - Binary output is common: hot/cold, default/not default, win/lose, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For now, assume $\\mathbb{Y} \\subseteq \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning: Model\n",
    "\n",
    "- Model: a family of functions indexed by a vector of parameters $\\theta$: $$\\hat{y} = f(x;\\theta)$$\n",
    "- Parameters $\\theta$ must be *learned*\n",
    "- Learning happens by examining a *loss function*\n",
    "    - Most common is **squared loss**: $$\\ell(y - \\hat{y}) = (y - \\hat{y})^2$$\n",
    "    - Sum of squared errors: $$SSE \\triangleq \\sum_{i=1}^N(y_i - \\hat{y}_i)^2$$\n",
    "    - Mean squared error (average loss): $$MSE(\\theta) = \\frac{1}{N} SSE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning: Learning\n",
    "\n",
    "- Loss function: $$MSE(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left(y_i \\hat{y}_i \\right)^2 = \\frac{1}{N} \\left\\| y - \\hat{y} \\right\\|^2$$\n",
    "- Learning algorithm\n",
    "    - Find $\\theta^*$ such that $\\hat{y}_i \\approx y_i$\n",
    "    - This is done by minimizing average training loss (or MSE) on training set\n",
    "    - Algorithm and approach should vary based on choice of loss function ($\\ell$) and model ($f$)\n",
    "- Prediction: $$\\hat{y}^* = f(x ; \\theta^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "Ingredients to supervised learning:\n",
    "\n",
    "1. Cleaned/transformed data $(x_i, y_i)_{i=1}^N$\n",
    "2. Choice of model $\\hat{y} = f(x; \\theta)$\n",
    "3. Choice of loss function $\\ell(y, \\hat{y})$\n",
    "4. Strategy for updating parameters $\\theta$ to minimize $\\ell(y, \\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models\n",
    "\n",
    "We'll focus now on regression problems using linear models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linearity\n",
    "\n",
    "First, a reminder:\n",
    "\n",
    "- Let $a_1, a_2 \\in \\mathbb{R}$, $x, x_1, x_2 \\in \\mathbb{X}$, $\\theta, \\theta_1, \\theta_2 \\in \\mathbb{\\theta}$\n",
    "-   A function $f: \\mathbb{X} \\times \\mathbb{\\theta} \\rightarrow \\mathbb{R}^N $ is linear...\n",
    "    -   in data ($x$) if $f(a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 | \\theta) = a_1 f(\\mathbf{x}_1|\\theta) + a_2 f(\\mathbf{x}_2|\\theta)$ \n",
    "    -   in parameters ($\\theta$) if $f(\\mathbf{x}|a_1 \\theta_1 + a_2 \\theta_2) = a_1 f(\\mathbf{x}|\\theta_1) + a_2 f(\\mathbf{x}|\\theta_2)$\n",
    "-   Without loss of generality a linear function $f$ can be represented as a\n",
    "    matrix equation\n",
    "-   The following model is linear in both data and parameters: $$f(x;\\theta) = x^T \\theta$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "- Spaces: $\\mathbb{Y}, \\mathbb{X}, \\mathbb{\\theta} \\subseteq \\mathbb{R}$\n",
    "- Model: $\\hat{y} = f(x; \\theta) = \\theta x$\n",
    "- Data: $(x, y)$ pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setting: Graphically\n",
    "\n",
    "- Model: All straight lines that pass through the origin\n",
    "\n",
    "![Simple Linear Regression](https://css-materials.s3.amazonaws.com/ML/linear_models_1/simple_linreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Average Training Loss\n",
    "\n",
    "- MSE on training set is \n",
    "\n",
    "\\begin{align*}\n",
    "    MSE(\\theta) & = \\frac{1}{N} \\sum_{i=1}^N \\left ( y_i - \\hat{y}_i \\right )^2 \\\\\n",
    "    &= \\frac{1}{N} \\sum_{i=1}^N \\left ( y_i - \\theta x_i \\right )^2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find $\\theta^* \\triangleq \\text{argmin}_\\theta MSE(\\theta)$, take derivative: \n",
    "\\begin{align*}\n",
    "\\frac{\\partial MSE}{\\partial \\theta} &= \\frac{2}{N} \\sum_{i=1}^N \\left(y_i - \\theta x_i \\right) x_i \\\\ \n",
    "&= \\frac{2}{N} \\left( x^T y - \\theta \\left\\|x\\right\\|^2\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Set $= 0$ and solve for $\\theta$: $$\\theta^* = \\frac{x^T y}{\\left\\|x \\right\\|^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "- Source: Acemoglu, Johnson and Robinson, 2001 (AJR)\n",
    "- Setting\n",
    "    - Goal of authors is to measure institutional differences and economic outcomes\n",
    "- Output: (`logpgp95`) Log GDP per capita in 1995 (USD equivalent)\n",
    "- Input (`avexpr`): index of protection against expropriation on average over 1985-95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_stata('https://github.com/QuantEcon/lecture-python/blob/master/source/_static/lecture_specific/ols/maketable1.dta?raw=true')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1.plot.scatter(x=\"avexpr\", y=\"logpgp95\", s=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df1_subset = df1.dropna(subset=['logpgp95', 'avexpr'])\n",
    "x = df1_subset[\"avexpr\"].to_numpy()\n",
    "y = df1_subset[\"logpgp95\"].to_numpy()\n",
    "theta_star = x @ y / (x @ x)\n",
    "yhat = x * theta_star\n",
    "mse_star = np.mean((y - yhat)**2)\n",
    "print(f\"The optimal coefficient is {theta_star:.3}\\nMSE(theta*) = {mse_star:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xgrid = np.linspace(min(x), max(x), 20)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(xgrid, theta_star*xgrid)\n",
    "ax.text(4, 11, r\"$\\hat{logpgp95} = 1.14 \\cdot avexpr$\")\n",
    "ax.set_xlabel(\"Protection against expropriation\")\n",
    "ax.set_ylabel(\"GDP per capita (1995, USD)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using statsmodels\n",
    "\n",
    "- We computed $\\theta^*$ by hand above\n",
    "- We could also use the `statsmodels` package\n",
    "- Here's how that could go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = smf.ols(\"logpgp95 ~ avexpr - 1\", data=df1)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comments\n",
    "\n",
    "- We saw that training amounts to computing the optimal $\\theta$ from the available, observed data\n",
    "- In the scenario we are considering, training is trivial in the sense that the optimal parameter value was found in closed form\n",
    "- The optimal parameter can even be found visually by graphing MSE vs. $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "th_grid = np.linspace(-2, 3, 100)\n",
    "# y: (N,) x: (N,) th_grid(100,)\n",
    "# Y[:, None]: (N, 1) x[:, None] (N, 1) th_grid: (100,)\n",
    "# th_grid*x[:, None] --> [N, 100]\n",
    "mse_grid = np.mean((y[:, None] - th_grid*x[:, None])**2, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(th_grid, mse_grid)\n",
    "ax.set_xlabel(r\"$\\theta$\")\n",
    "ax.set_ylabel(\"MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "- Data: $\\mathbb{X}, \\mathbb{\\theta} \\subseteq \\mathbb{R}^D$, $\\mathbb{Y} \\subseteq \\mathbb{R}$\n",
    "- Model: $\\hat(y) = f(x; \\theta) = x^T \\theta$, all hyperplanes that pass through origin\n",
    "- Predictions on samples:\n",
    "\\begin{align*}\n",
    "\\hat{y} = \\begin{bmatrix}\\hat{y}_1 \\\\ \\cdots \\\\ \\hat{y}_N\\end{bmatrix}\n",
    "    = \\begin{bmatrix} x_1^T \\theta \\\\ \\cdots \\\\ x_N^T \\theta \\end{bmatrix}\n",
    "    = \\underbrace{\\begin{bmatrix} x_1^T  \\\\ \\cdots \\\\ x_N^T  \\end{bmatrix}}_{\\triangleq X}\\theta\n",
    "    = X \\theta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MSE\n",
    "\n",
    "\n",
    "- In this setting the MSE is: \n",
    "\\begin{align*}\n",
    "MSE(\\theta) &= \\frac{1}{N} \\left\\|x - X \\theta \\right\\|^2 \\\\\n",
    "&= \\frac{1}{N} (y - X\\theta)^T(y - X \\theta) \\\\\n",
    "&= \\frac{1}{N} \\left(y^T y - 2 \\theta^T X^T y + \\theta ^T \\underbrace{X^T X}_{\\equiv R} \\theta \\right)\n",
    "\\end{align*}\n",
    "- Take derivative wrt $\\theta$: $$\\frac{\\partial MSE}{\\partial \\theta} = \\frac{1}{N} \\left(-2 X^T y + 2 R\\theta \\right)$$\n",
    "- Solve for $\\theta^*$: $$\\theta*^ = \\underbrace{R^{-1}X^T}_{\\equiv X^{\\dagger}} y = X^{\\dagger} y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Comments about $R$\n",
    "\n",
    "\n",
    "We have $$R \\triangleq X^TX \\in \\mathbb{R}^{D \\times D}$$\n",
    "\n",
    "- If $N < D$ (more features than samples) then $R$ will not be invertible\n",
    "- In this case $\\theta^*$ is not uniquely identified\n",
    "    - Example $N=1, D=2$\n",
    "    - There are infinitely many planes that pass through the origin and one sample\n",
    "    - In all cases MSE = 0\n",
    "- Therefore, we need at least one sample per features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Comments about $X^{\\dagger}$\n",
    "\n",
    "\n",
    "We have $$X^{\\dagger} \\triangleq R^{-1}X^T = (X^TX)^{-1}X^T \\in \\mathbb{R}^{D \\times N}$$\n",
    "\n",
    "- Called the Moore-Penrose Pseudo-Inverse of $X$\n",
    "- Consider the system of equations: $$y = X \\theta$$\n",
    "- If $X$ is square (and invertible), we can solve $\\theta = X^{-1} y$\n",
    "- If $X$ has more rows than columns (more samples than features),  then $X^{-1}$ is not defined\n",
    "    - In this setting we use $X^{\\dagger}$ as if it were $X^{-1}$ and compute $\\theta = X^{\\dagger} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "- Let's extend the AJR example to include an intercept term\n",
    "- The model we will fit is $$\\hat{\\text{logpgp95}} = \\theta_0 + \\theta_1 \\text{avexpr}$$\n",
    "- In this case, our matrix $X$ is $$X \\equiv \\begin{bmatrix} 1 & \\text{avexpr} \\\\ 1 & \\text{avexpr}_2 \\\\ \\vdots & \\vdots \\\\ 1 & \\text{avexpr}_N \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model2 = smf.ols(\"logpgp95 ~ 1 + avexpr\", data=df1_subset)  # or formula \"logpgp95 ~ avexpr\"\n",
    "fit2 = model2.fit()\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the readout above we can say that the optimal linear regression model for this probem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "theta2 = fit2.params\n",
    "print(f\"logpgp95 = {theta2[0]:.3} + {theta2[1]:.3} avexpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prediction\n",
    "\n",
    "We can use our model to make predictions about the level of GDP per capita, given a specific level of the expropriation protection index\n",
    "\n",
    "For example, consider a country with an average level of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mean_expr = df1[['avexpr']].mean()\n",
    "mean_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_logpdp95 = 4.63 + 0.53 * 7.07\n",
    "predicted_logpdp95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An easier way to do this would be to use the `.predict` method on our fit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fit2.predict(mean_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we don't pass any arguments to `.predict`, it will compute $\\hat{\\text{logpgp95}}$ for all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df1_subset.plot.scatter(x=\"avexpr\", y=\"logpgp95\", ax=ax, s=40)\n",
    "df1_subset.assign(yhat=fit2.predict()).plot.scatter(x=\"avexpr\", y=\"yhat\", ax=ax, c=\"g\", s=40)\n",
    "ax.legend([\"observed\", \"predicted\"], loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using sklearn\n",
    "\n",
    "- `statsmodels` is a great package for our linear regression example\n",
    "- However, for other problems, a dedicated ML package may suit our needs better\n",
    "- scikit-learn (`sklearn`) is the go-to Python package for classic ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sklearn Model API\n",
    "\n",
    "- One of the best features of sklearn is its well designed and consistent API\n",
    "- All models in sklearn share the `BaseEsimator` api\n",
    "- The two core methods are `.fit(X, y)` and `.predict`\n",
    "- Let's replicate our example above using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first step is to create a model instance\n",
    "\n",
    "This is where we pick a model $f$ from our notation\n",
    "\n",
    "> Note, no data is passed at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "skmodel = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The next step is to fit the model on (X, y) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = df1_subset[[\"avexpr\"]]\n",
    "y = df1_subset[\"logpgp95\"]\n",
    "skmodel.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can inspect coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(skmodel.intercept_, skmodel.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare to statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fit2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can use the `predict` method to estimate `logpgp95` for an arbitrary `avexpr`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "skmodel.predict([mean_expr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.scatter(X, skmodel.predict(X))\n",
    "ax.legend([\"observed\", \"predicted\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sklearn metrics\n",
    "\n",
    "- sklearn implements many models and algorithms as estimators\n",
    "- It also has an extensive set of loss functions/metrics in the `metrics` subpackage\n",
    "- The common API for each of these metrics is `metrics.metric_name(y, yhat)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's evaluate the MSE using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_squared_error(y, skmodel.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.mean((y - skmodel.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aside: Computing $\\theta^*$ in pratice\n",
    "\n",
    "- We saw above that we can solve for $\\theta^*$ by:\n",
    "    1. Computing $R = X^T X$\n",
    "    2. Inverting $R$ to get $R^{-1}$\n",
    "    3. Computing $X^{\\dagger} = R^{-1}X^T$\n",
    "    4. Computing $\\theta^* = X^{\\dagger} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, in practice we compute $\\theta*$ by iteratively minimizing the MSE\n",
    "- This is more numerically stable than the direct approach (espeically inverting $R$)\n",
    "- It also might be the only feasible approach if the data is \"big\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tools: Fitted residual plots and polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One important skill is to be able to assess the goodness of fit for a model\n",
    "- Ultimately the quality of fit is a function of the application of the model\n",
    "- There are some common tools that can help make the evaluation process less subjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Fitted residual plots\n",
    "\n",
    "- For a model $f$, the *residual* for observation $i$ is $\\epsilon_i \\equiv y_i - f(x_i ; \\theta)$\n",
    "- Sometimes there can be a relationship between $y_i$ and $\\epsilon_i$\n",
    "- If this relationship is observable/obvious, it can prompt researcher to choose a more appropriate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "- Data: from the built in `women` dataset from R language\n",
    "- 15 observations of height and weigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"https://css-materials.s3.amazonaws.com/ML/linear_models_1/women.csv\")\n",
    "df2[[\"height\", \"weight\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear fit looks quite accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "modelw = smf.ols(\"weight ~ height\", data=df2)\n",
    "fitw = modelw.fit()\n",
    "fitw.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"mse = {fitw.ssr / df2.shape[0]:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# plot data\n",
    "df2.plot.scatter(x=\"height\", y=\"weight\", ax=ax)\n",
    "\n",
    "\n",
    "# plot model\n",
    "grid_height = np.linspace(df2[\"height\"].min(), df2[\"height\"].max(), 20)\n",
    "ax.plot(grid_height, fitw.predict(pd.Series(grid_height, name=\"height\")))\n",
    "\n",
    "# lables\n",
    "ax.set_xlabel(\"height (in)\")\n",
    "ax.set_ylabel(\"weight (lbs)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now take a look at the fitted residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "epsw = fitw.resid\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df2[\"weight\"], epsw)\n",
    "ax.set_xlabel(\"weight(lbs)\")\n",
    "ax.set_ylabel(r\"$\\hat{\\epsilon}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe a clear quadratic pattern\n",
    "\n",
    "This might suggest that we should add a quadratic term to our regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "modelw2 = smf.ols(\"weight ~ height + I(height*height)\", data=df2)\n",
    "fitw2 = modelw2.fit()\n",
    "fitw2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"mse = {fitw2.ssr / df2.shape[0]:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# plot data\n",
    "df2.plot.scatter(x=\"height\", y=\"weight\", ax=ax)\n",
    "\n",
    "\n",
    "# plot model\n",
    "grid_height = np.linspace(df2[\"height\"].min(), df2[\"height\"].max(), 20)\n",
    "ax.plot(grid_height, fitw2.predict(pd.Series(grid_height, name=\"height\")))\n",
    "\n",
    "# lables\n",
    "ax.set_xlabel(\"height (in)\")\n",
    "ax.set_ylabel(\"weight (lbs)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "epsw2 = fitw2.resid\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df2[\"weight\"], epsw2)\n",
    "ax.set_xlabel(\"weight(lbs)\")\n",
    "ax.set_ylabel(r\"$\\hat{\\epsilon}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fitted residual plot looks like random noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Comments\n",
    "\n",
    "- Note that the model including the quadratic height squared term is still a *linear* regression\n",
    "- When speaking about regression models, linearity refers to the parameters, not the data\n",
    "- We can apply arbitrary transformations to $x$ before multiplying by $\\theta$ to produce a prediction\n",
    "- Much of the art of applied ML is choosing the transformations wisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Our linear regression framework can be extended to predict a binary outcome\n",
    "- In this case $\\mathbb{Y} =  \\{0,1 \\}$\n",
    "- The key to using this framework is change one assumption:\n",
    "    - Instead of assuming $y$ is linear in $x$, $\\theta$...\n",
    "    - We assume that the log odds are linear in $x, \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Log Odds\n",
    "\n",
    "- Let $Y$ be a Bernoulli distributed random variable\n",
    "- Let $p = P(Y = 1)$ represent the probability that $Y = 1$\n",
    "- The *odds* that $Y = 1$ are given by $\\frac{p}{1-p}$\n",
    "- The *logistic regression* framework assumes that the log odds are linear in $\\theta$: $$\\log \\frac{\\hat{y}_i}{1-\\hat{y}_i} = x^T \\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Logistic Function\n",
    "\n",
    "\n",
    "- If we re-arrange the expression of log-odds, we can solve for $$\\hat{y} = \\frac{\\exp(x^T\\theta)}{1 + \\exp(x^T\\theta)}$$\n",
    "- This is known as the *logistic function*, which is where logistic regression gets its name\n",
    "- The logistic function has a range of $[0, 1]$, making it ideal for representing probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "ax.plot(x, logistic(x))\n",
    "ax.set_ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: credit card default\n",
    "\n",
    "- We'll come back to how to compute the coefficients of this model in a later lecture\n",
    "- For now, we will look at an example and lean on sklearn to do the computations\n",
    "- Data: 10,000 observations of inidividual income and credit card balance\n",
    "- From \"An Introduction to Statistical Learning: With Applications in R\" by James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"https://phbs-css.s3-ap-southeast-1.amazonaws.com/Module3/L04_linear_models/default.csv\")\n",
    "df3.info()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# convert to boolean instead of strings \"Yes\" and \"No\"\n",
    "df3[\"default\"] = df3[\"default\"] == \"Yes\"\n",
    "df3[\"student\"] = df3[\"student\"] == \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To start, let's fit a linear regression for the default probability as a function of the credit card balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "linreg = linear_model.LinearRegression()\n",
    "\n",
    "# prep data\n",
    "X = df3[[\"balance\"]]\n",
    "y = df3[\"default\"]\n",
    "\n",
    "# fit model\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bal_grid = np.linspace(X.min(), X.max(), 500)\n",
    "ax.scatter(X, y, alpha=0.3, s=10)\n",
    "ax.scatter(bal_grid, linreg.predict(bal_grid), s=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that when the credit card balance is between 0 and 500, the predicted probability of default is negative\n",
    "- This violates the definition of a probability, showcasing why linear regression is not the right model for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try fitting the loigistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, alpha=0.3, s=10)\n",
    "ax.scatter(bal_grid, logreg.predict(bal_grid), s=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Notice now that the predicted probabilities are squarely between 0 and 1"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
