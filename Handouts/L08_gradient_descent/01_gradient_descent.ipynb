{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5c9334",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Numpy\n",
    "- Calculus\n",
    "- Linear Algebra\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the gradient descent algorithm for unconstrained optimization\n",
    "- Implement gradient descent using analytical derivatives\n",
    "- Understand finite difference methods of approximating derivatives\n",
    "- Be introduced to tensorflow and the concept of autodifferentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31addf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c68ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Nonlinear Optimization\n",
    "\n",
    "- The class of nonlinear optimization problems is very broad\n",
    "- The main objective:\n",
    "$$\\begin{align}\n",
    "\\min_x \\quad & f(x) \\\\\n",
    "\\text{s.t.}\\quad & c_E(x) = 0 \\\\\n",
    "& c_I(x) \\le 0\n",
    "\\end{align}$$\n",
    "- We'll focus on the *unconstrained problem*: $$\\min_x f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cda96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- First order (uses first derivative -- gradient) for finding **local minimum** of a **differentiable** function $$f: X \\rightarrow \\mathbb{R}$$\n",
    "- We assume $X \\subseteq \\mathbb{R}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b00a85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Reminders\n",
    "\n",
    "- Recall: a function $f$ is minimized (or maximized!) where $\\frac{df}{dx} = 0$\n",
    "- Also recall that the *gradient* of $f$ is: $$\\nabla f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_N}\\end{bmatrix}$$\n",
    "- $\\nabla f \\big|_{x = \\hat{x}}$ gives the direction of greatest *increase* in $f$, starting from the point $x = \\hat{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4287fdc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Intuition\n",
    "\n",
    "- If $\\nabla f$ represents direction of greatest increase in $f$...\n",
    "- then moving *away* from this point will move us in a direction of greatest decrease\n",
    "- The gradient descent algorithm is an iterative algorithm that builds on this insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### The Algorithm\n",
    "\n",
    "The gradient descent algorithm proceeds as follows:\n",
    "\n",
    "- Initialization:\n",
    "    - Choose a starting point $x_0$,\n",
    "    - convergence tolerance $\\epsilon$,\n",
    "    - maximum number of iterations $T$,\n",
    "    - step size $\\alpha$\n",
    "- Iteration: for iteration $i = 0, \\cdots T$\n",
    "    1. Evaluate $dx_i \\equiv \\nabla f(x)\\big|_{x=x_i}$\n",
    "    2. Compute $x_{i+1} = x_i - \\alpha dx_i$\n",
    "    3. Check $\\rho(dx_i) < \\epsilon \\quad \\big | \\quad i = T$\n",
    "    \n",
    "        - If True, stop and return $x_{i+1}$\n",
    "        - If False, continue to next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a641ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Gradient Descent By Hand\n",
    "\n",
    "Below we have written a very direct implementation of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(df, x0, epsilon=1e-3, T=200, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Given a gradient function df, staritng starting point x0,\n",
    "    stopping parameters epsilon and T, and a learning rate alpha;\n",
    "    find a local minimum of f(x) near x_0 via gradient descent\n",
    "    \"\"\"\n",
    "    x = np.copy(x0)\n",
    "    trace = []\n",
    "    for i in range(T):\n",
    "        df_i = df(x)\n",
    "        xp = x - alpha * df_i\n",
    "        err = max(abs(df_i))\n",
    "        status = {\"x\": xp, \"i\": i, \"err\": err}\n",
    "        trace.append(status)\n",
    "        if err < epsilon:\n",
    "            return trace\n",
    "        x[:] = xp[:]\n",
    "\n",
    "    raise ValueError(\"Failed to converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98466903",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To test our algorithm, let's define a function `f` for which we know the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401868c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -np.exp(-(x[0]**2 + x[1]**2))\n",
    "\n",
    "def f2(x):\n",
    "    return  -np.sinc(x[0]/2)-np.exp(-((x[0]-1)**2 + (x[1]-1/2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de7816",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_surf(f, **kw):\n",
    "    L, n = 2, 400\n",
    "    x = np.linspace(-L, L, n)\n",
    "    y = x.copy()\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # set up 3d plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': '3d'})\n",
    "\n",
    "    Z = f([X, Y])\n",
    "\n",
    "    ax.plot_surface(X, Y, Z, cmap=\"viridis\", **kw)\n",
    "    return ax\n",
    "\n",
    "\n",
    "plot_surf(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd84d0e",
   "metadata": {},
   "source": [
    "As you can see from the above we have a global minimum at (0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa69c2",
   "metadata": {},
   "source": [
    "Let's define a function that can compute the gradient of our function: $$\\nabla f(x) = \\begin{bmatrix} -2 x_1 f(x) \\\\ -2 x_2 f(x) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b36ea4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return -2*np.asarray(x)*f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fedbed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "And now we can use our `grad_descent` function to find the minimizer for the function `f`, given its gradient `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = grad_descent(df, [2, -0.3])\n",
    "opt = trace[-1]\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd78a73",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's redo the surface plot, this time including a scatter chart showing the path of $x_i$ over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_xyz(f, trace):\n",
    "    xy = [i[\"x\"] for i in trace]\n",
    "    x, y = zip(*xy)\n",
    "    z = f([np.array(x), np.array(y)])\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def plot_path(f, trace, **kw):\n",
    "    ax = plot_surf(f, **kw)\n",
    "    x, y, z = get_trace_xyz(f, trace)\n",
    "    ax.scatter3D(x, y, z, c=\"red\")\n",
    "    ax.plot(x, y, z, c=\"red\")\n",
    "    ax.view_init(10, 10*7)\n",
    "    return ax\n",
    "\n",
    "plot_path(f, trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67c0f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Another very helpful way to view the algorithm's progress is with a contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour_path(f, trace, ax=None):\n",
    "    L, n = 2, 400\n",
    "    x = np.linspace(-L, L, n)\n",
    "    y = x.copy()\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f([X, Y])\n",
    "\n",
    "    # set up plot\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, inline=True, fontsize=10)\n",
    "    \n",
    "    x, y, z = get_trace_xyz(f, trace)\n",
    "    ax.scatter(x, y, c=np.linspace(0.5, 1, len(x)), s=8)\n",
    "    ax.set_title(\"Convergence in {} iterations\".format(len(x)));\n",
    "    return ax\n",
    "\n",
    "ax = plot_contour_path(f, trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848e0da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Importance of Learning rate\n",
    "\n",
    "- Recall the update rule for $x$: $x_{i+1} = x_i - \\alpha \\nabla f(x) \\big|_{x=x_i}$\n",
    "- The parameter $\\alpha$ is called the *learning rate*\n",
    "- This influences two key features of the algorithm:\n",
    "    - *rate of convergence*: how many iterations it takes to go from starting position to final position\n",
    "    - *stability of iterations*: whether the problem remains well defined from one iteration to the next\n",
    "- In general a smaller $\\alpha$ means a slower rate of convergence, but more stable algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d9e07",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Small Learning rate\n",
    "\n",
    "- Let's see what happens as we make the learning rate very small\n",
    "- By default, or `grad_descent` sets $\\alpha$ = 0.1\n",
    "- Let's try smaller values for $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a48dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alpha_experiment(alphas):\n",
    "    N = len(alphas)\n",
    "    fig, ax = plt.subplots(1, N, figsize=(N*4, 4))\n",
    "    for alpha, ax in zip(alphas, ax):\n",
    "        trace_alpha = grad_descent(df, [2, -0.3], alpha=alpha, T=100_000)\n",
    "        plot_contour_path(f, trace_alpha, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550071fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha_experiment([0.1, 0.01, 0.0001]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76587f39",
   "metadata": {},
   "source": [
    "- Notice how we ended up at the same place (near (0,0)), but when alpha became very small it took far more iterations\n",
    "- Also notice that when the slope gets steeper (between (0, 1) for example), the steps are larger\n",
    "    - This is a feature of gradient descent: when gradient is large (steep), steps are bigger\n",
    "    - Easiest to see on chart where alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991b665",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Impact of high learning rate\n",
    "\n",
    "- Let's now consider the alternative where $\\alpha$ is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c42e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_experiment([0.1, 0.5, 0.95]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02adc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Notice that with $\\alpha = 0.5$ convergence was very fast -- only 16 iterations\n",
    "- However, when we increase alpha further to $\\alpha=0.95$ we needed 64 iterations, why?\n",
    "    - Learning rate was too high and caused overshoot\n",
    "    - Can think of gradient descent as rolling a marble on a surface with a \"push\" and a \"snapshot\" at fixed time intervals\n",
    "        - Strength of push related to size of $\\alpha$. If you push too hard, you overshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcb9cb",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace95 = grad_descent(df, [2.0, -0.3], alpha=0.95)\n",
    "plot_path(f, trace95, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046372fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The problem can get worse if we make $\\alpha$ too big:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent(df, [0.2, -0.3], alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c66a2",
   "metadata": {},
   "source": [
    "- In this case the iterations became unstable and the marble \"escaped\"\n",
    "- To plot the escape we need to do a few iterations \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4422a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interactive_alpha_experiment(alpha_power, T=50):\n",
    "    \"alpha = 10**alpha_power to make it easy to do things on log10 scale\"\n",
    "    x0 = np.array([2.0, -0.3])\n",
    "    x = np.copy(x0)\n",
    "    trace = []\n",
    "    \n",
    "    alpha = 10 ** alpha_power\n",
    "\n",
    "    for i in range(T):\n",
    "        df_i = df(x)\n",
    "        xp = x - alpha * df_i\n",
    "        err = max(abs(df_i))\n",
    "        status = {\"x\": xp, \"i\": i, \"err\": err}\n",
    "        trace.append(status)\n",
    "        x[:] = xp[:]\n",
    "\n",
    "    ax = plot_path(f, trace, alpha=0.3)\n",
    "    ax.set_title(f\"alpha = {alpha}\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47535ece",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "interact(\n",
    "    interactive_alpha_experiment, \n",
    "    alpha_power=widgets.FloatSlider(min=-3, max=3, step=0.25, value=-1),\n",
    "    T=widgets.IntSlider(min=1, max=1000, step=1, value=50)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f92e45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Numerical Derivatives\n",
    "\n",
    "- In our example above, we chose an easily differentiable function\n",
    "- We will not always be fortunate enough to know the derivative of our objective function in closed form\n",
    "- In these cases, the` standard approach is to approximate the derivative numerically\n",
    "- The classic algorithm for numerical approximation of derivatives is called finite differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e4b2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Finite Differences\n",
    "\n",
    "- A finite difference approximation of a derivative comes directly from the definition of a derivative: $$\\frac{df}{dx} = \\lim_{\\delta \\downarrow 0} \\frac{f(x + \\delta) - f(x)}{\\delta}$$\n",
    "- In code, we can choose a value for $\\delta$ that is very small -- perhaps on the order of 1e-6 -- and evaluate the fraction above\n",
    "- For the gradient, we apply the finite difference approximation one element of $x$ at a time: $$\\nabla f(x) \\approx \\begin{bmatrix}\\frac{f(x + e_1 \\delta) - f(x)}{\\delta} \\\\ \\frac{f(x + e_2 \\delta) - f(x)}{\\delta} \\\\ \\vdots \\\\ \\frac{f(x + e_N \\delta) - f(x)}{\\delta} \\end{bmatrix},$$\n",
    "where $e_i$ is the $i$th unit vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9edf53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def forward_difference(f, x, delta):\n",
    "    out = np.zeros_like(x)\n",
    "    fx = f(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        xi = np.copy(x)\n",
    "        xi[i] += delta\n",
    "        fx_i = f(xi)\n",
    "        out[i] = (fx_i - fx) / delta\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488e658",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.array([0.2, 0.4])\n",
    "forward_difference(f, x0, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e53e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent_finite_diff(f, x0, delta=1e-4, **kw):\n",
    "    def df_fd(x):\n",
    "        return forward_difference(f, x, delta=delta)\n",
    "    return grad_descent(df_fd, x0, **kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71841b03",
   "metadata": {},
   "source": [
    "- Let's see what happens in our example when we use numerical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_fd = grad_descent_finite_diff(f, [2, -0.3])\n",
    "trace_fd[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Choosing $\\delta$\n",
    "\n",
    "- To use finite differencing techniques we need to choose a value for the parameter $\\delta$\n",
    "- In the mathematical theory, $\\delta$ should approach zero to compute the exact derivative (remember the limit definition!!)\n",
    "- However, computers don't do exact artithmetic\n",
    "- Instead, they use floating poing approximations\n",
    "- One implication of this is that dividing by a very small number (like a small $\\delta$) can be highly inaccurate\n",
    "- Let's explore this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fd_err(f, df, x0):\n",
    "    x = []\n",
    "    y = []\n",
    "    dfdx = df(x0)\n",
    "    for delta in np.logspace(-15, 0, 70):\n",
    "        approx_dfdx = forward_difference(f, x0, delta=delta)\n",
    "        x.append(delta)\n",
    "        y.append(max(abs(dfdx - approx_dfdx)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.loglog(x, y)\n",
    "    ax.set_xlabel(\"delta\")\n",
    "    ax.set_ylabel(\"abs error in âˆ‡f\")\n",
    "    return ax\n",
    "\n",
    "plot_fd_err(f, df, [0.5, -0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79059875",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Aside: Smoothness\n",
    "\n",
    "- The function `f` we have been working with is particularly well behaved\n",
    "- Many objective functions in machine learning are not\n",
    "- When we have a less smooth function, choosing $\\delta$ is even more difficult and important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3c407",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Comment on Efficiency\n",
    "\n",
    "- Note that in order to do a finite difference approximation of the gradient we needed N+1 function calls, where $N$ is the number of elements in $x$\n",
    "- This is ok when $N = 2$\n",
    "- But when we do deep learning, $N$ can be the number of parameters in our model, which can be in the billions!\n",
    "- Trying to do analytical (by hand) derivatives of this function is not feasible, and trying to use finite difference techniques is prohibitively costly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df80823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "- Up until this point we have leaned on the numpy and pandas libraries\n",
    "- They are very powerful and excellent at what they do\n",
    "- However, as we build towards more computationally expensive models (like deep neural networks), we will need to power up our tools\n",
    "- There are a few enterprise quality libraries specializing in this type of computation\n",
    "- They are tensorflow (from Google) and pytorch (from Facebook)\n",
    "- We will use tensorflow in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6209fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensorflow Overview\n",
    "\n",
    "- At its heart tensorflow has a few core features:\n",
    "    - n-dimensional array or **tensor** (similar to numpy)\n",
    "    - ability to execute on specialized hardware (like GPU or TPU)\n",
    "    - rich set of functions useful for computation, machine learning, and deep learning\n",
    "    - support for deploying ML models to production and running on *edge* devices\n",
    "    - *automatic differentiation*\n",
    "- This last point is one we'll focus on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2a8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "- There are 3 ways computers can compute (or approximate) derivatives:\n",
    "    1. Numerical approximation: like finite differences from above\n",
    "    2. Symbolic differentiation: represent variables as symbols and use a rules engine to compute derivatives\n",
    "    3. Automatic differentiation: compute *exact* derivatives *numerically*\n",
    "- Various benefits of automatic differentiation\n",
    "    - Efficient: same order of complexity (\"big O\") as evaluating the function\n",
    "    - Flexible: computes derivative of what happened, allows `for`, `if`, etc.\n",
    "    - Exact: not an approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7e779",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We will see how to use the autodiff functionality of tensorflow, but first we need to start at the basics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f039c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to install tensorflow\n",
    "# %pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a69279",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many of our familiar numpy array creation funcitons have been replicated in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83228733",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linspace(-2, 2, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282b4ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.ones((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b85d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.zeros((2, 3, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b12e1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1 = tf.eye(3)\n",
    "x2 = tf.reshape(tf.range(1, 10, dtype=\"float32\"), (3,3))\n",
    "\n",
    "tf.sin(x1) + tf.exp(x2) * (x1+1) / (x2**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d473da9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Notice above we had to be careful to specify that we wanted tensors with a floating point dtype\n",
    "- Tensorflow is more *low level* than numpy is, and will sometimes require extra care with things like types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e55730",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also mix and match numpy arrays and tensorflow tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b23ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1np = np.eye(3)\n",
    "\n",
    "x1np + x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689cf2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sin(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7061d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can get a numpy array  using .numpy() method\n",
    "x1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ada4f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Autodiff in tensorflow\n",
    "\n",
    "- Now that we are warmed up, let's see how we can do autodiff in tensorflow\n",
    "- First we need to redefine our objective function to use `tf.exp` instead of `np.exp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_tf(x):\n",
    "    return -tf.exp(-(x[0]**2 + x[1]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdccc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Then, tensorflow needs to know we would like it to track the derivatives for us\n",
    "- To do this we have to create a tensorflow `Variable`\n",
    "- We also need to create what is called a `GradientTape`\n",
    "    - The name `Tape` comes from automatic differentiation theory, not crucial for us right now\n",
    "    - The tape will record all operations that happen to `Variable`s\n",
    "    - We can then differentiate values with respect to `Variable`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecd225",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x0_tf = tf.Variable([0.2, -0.3])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    val = f_tf(x0_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6611207",
   "metadata": {},
   "outputs": [],
   "source": [
    "tape.gradient(val, x0_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292440f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x0_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97c538",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent with Tensorflow and Autodiff\n",
    "\n",
    "- We are now ready to put it all together and use tensorflow + autodiff to implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf48b02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc_tf(f, x0, epsilon=1e-3, T=200, alpha=0.1):\n",
    "    \n",
    "    trace = []\n",
    "    x = tf.Variable(x0)\n",
    "    for i in range(T):\n",
    "        with tf.GradientTape() as tape:\n",
    "            fx = f(x)\n",
    "        dfdx = tape.gradient(fx, x)\n",
    "        xp = x - alpha * dfdx\n",
    "        err = max(abs(dfdx))\n",
    "        status = dict(\n",
    "            i=i, \n",
    "            fx=fx.numpy(), dfdx=dfdx.numpy(), \n",
    "            err=err.numpy(), x=x.numpy()\n",
    "        )\n",
    "        trace.append(status)\n",
    "        if err < epsilon:\n",
    "            return trace\n",
    "        \n",
    "        x = tf.Variable(xp)\n",
    "    \n",
    "    raise ValueError(\"No convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83c21e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trace_tf = grad_desc_tf(f_tf, [2, -0.3])\n",
    "trace_tf[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58caa8b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- We learned more about the gradient descent algorithm today\n",
    "- Gradient descent (and its relatives) are at the heart of many modern deep learning algorithms\n",
    "- We saw the importance of the important learning rate parameter and how it relates to the rate of convergence and the stability of the algorithm\n",
    "- We also learned about tensorflow and automatic differentiation\n",
    "- We will continue using the gradient descent knowlege and tensorflow library as we move forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046d089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
