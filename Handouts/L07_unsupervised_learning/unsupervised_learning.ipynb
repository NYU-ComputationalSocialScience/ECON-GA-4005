{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fd1ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Supervised Learning\n",
    "- Linear Algebra\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the principal component analysis algorithm for dimensionality reduction\n",
    "- Understand the k-means algorithm for clustering\n",
    "\n",
    "> Note: Parts of this notebook were inspired by sections from [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "color_cycle = np.array(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e48e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "ML Families:\n",
    "\n",
    "| Family | Data | examples | \n",
    "| ------ | ---- | -------- |\n",
    "| Supervised | (X, y) | regression/classification |\n",
    "| **Unsupervised** | (X,) | clustering/dimensionality reduction/etc | \n",
    "| Reinforcement | (s, **a**, R, s', **a'**, R'...) | games/robots/financial trading/\"control\" |\n",
    "\n",
    "> Note: also semi-supervised, where some X's have a corresponding y, but not all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20922a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "\n",
    "- Today we will focus on unsupervised learning\n",
    "- The overarching idea: have computer discover structure in unlabeled data\n",
    "- Algorithms we will study:\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - K-means Clustering\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1e754",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- PCA, at its heart, is primarily an algorithm for *dimensionality reduction*\n",
    "- This means it is effective at summarizing key varaition in high dimensional data using fewer dimensions\n",
    "- However, it can also be used for many tasks:\n",
    "    - Feature engineering\n",
    "    - Visualization\n",
    "    - Noise removal/filtering\n",
    "- We will focus on dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b5e20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Principal Components\n",
    "\n",
    "- The PCA algorithm represents the relationship between columns of X using the *principal components* of X\n",
    "    - *Note*: principal components are sometimes called principal axes\n",
    "- A principal component is a vector of the same dimensionality as $x$ (a row of $X$) that summarizes the direction of greatest variation in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3df2ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Graphical Example\n",
    "\n",
    "- PCA is (perhaps) best understood visually\n",
    "- We'll first show a graphical example then describe the math behind algorithm\n",
    "- Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0988fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "fig, ax = plt.subplots()\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bae82",
   "metadata": {},
   "source": [
    "- Clear linear relationship between x1 and x2\n",
    "- In supervised learning...\n",
    "    - we might build a model $f(x1;\\theta)$ such that $x2 \\approx f(x1;\\theta)$\n",
    "    - In words: we may try to **predict** x2 given x1\n",
    "- In unsupervised learning we instead want to learn the relationship between x1 and x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b63cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "Xhat = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862b25e",
   "metadata": {},
   "source": [
    "- When fit is called, the PCA algorithm learns two things:\n",
    "    1. `pca.components_`: matrix whose columns are principal components\n",
    "    2. `pca.explained_variance_` (or `pca.explained_variance_ratio_`): variance explained by the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843a0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"EVR:\", pca.explained_variance_ratio_)\n",
    "print(\"PCs:\\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8b3f3",
   "metadata": {},
   "source": [
    "- This tells us that the direction of highest variance is -0.94 x1 and -0.32 x2\n",
    "    - In other words, 3 times more variance is explained by x1 than x2\n",
    "- The explained variance ratio tells us that 97% of the variance is explained by the first principal component\n",
    "- Let's plot these components alongside the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d038c45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2, color=\"black\",\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0dfc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Computing Principal Components\n",
    "\n",
    "- The most intuitive algorithm for doing PCA is done iteratively\n",
    "- Before iterating set $i=1$, $\\tilde{X}_1 = X$, and choose the number of components K\n",
    "- Then, on iteration $i$:\n",
    "    1. Find a unit vector $w_i$ that maximizes the variance of a projection of $X$ onto $w_i$\n",
    "        1. Mathematically can be done by choosing $w_i$ to maximize $\\|\\tilde{X}_i w_i\\|$ (see below for $\\tilde{X}_i$)\n",
    "        1. Because $w_i$ must have norm 1, this is equivalent to $$w_i = \\text{argmax}_w \\frac{w^TX^TXw}{w^Tw}$$\n",
    "    1. After choosing $w_i$, compute $\\tilde{X}_{i+1}$: $$\\tilde{X}_{i+1} = X - \\sum_{s=1}^{i} X w_s w_s^T$$\n",
    "    1. If $i< K$ continue to next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c015d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's implement this algorithm below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0dffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def pca_iterative(X, K):\n",
    "    vectors = []\n",
    "    Xtilde = X\n",
    "    for _ in range(K):\n",
    "        xhat = Xtilde.T @ Xtilde\n",
    "        def obj(w):\n",
    "            return - w@xhat@w / (w@w)\n",
    "        \n",
    "        w = minimize(obj, 0.1*np.ones(X.shape[1])).x  # find w\n",
    "        w /= np.linalg.norm(w)                        # make unit vector\n",
    "        vectors.append(w)                             # store w_i\n",
    "        Xtilde = Xtilde - X @ np.outer(w, w)          # update Xtilde\n",
    "    \n",
    "    return np.column_stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7faf23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Iterative PCs:\\n\", pca_iterative(X, 2))\n",
    "print(\"pca.components_:\\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f1b49",
   "metadata": {},
   "source": [
    "Note: a projection on $[0.944, 0.328]$ will be the same as one on $[-0.944, -0.328]$, so these principal components are equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05581fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Comments\n",
    "\n",
    "- The algorithm we implemented above was descriptive, but quite inefficient\n",
    "- On each iteration we used a non-linear minimizer to find $w_i$\n",
    "- Using some linear algebra tricks, we could have computed the principal components much more succinctly..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46afa5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### PCA via eigen decomposition\n",
    "\n",
    "- For a matrix $X$ whose columns have mean 0, the matrix $X^TX$ is the covariance matrix of $X$\n",
    "- Principal components end up being eigenvectors of covariance matrix\n",
    "    - Explained variance is associated eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ae7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = X.T@X\n",
    "vals, vecs = np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df398ede",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"vecs:\\n\", vecs)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44506878",
   "metadata": {},
   "source": [
    "- Recall that eigenvalues are only unique up to a scalar multiple\n",
    "- Multiplying the first column of `vecs` by `-1` gives the components from PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff4342",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Also, if we divide `vals` by its sum, we get the explained variance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3515eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"vals / vals.sum():\", vals/vals.sum())\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc33f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### PCA via SVD\n",
    "\n",
    "- By more linear algebra magic (trust us!), we can also do PCA via the singular value decomposition\n",
    "- Recall that the SVD of a matrix $X$ is $$X = U \\Sigma W^T,$$\n",
    "    - $U$: n by n matrix whose columns are orthogonal unit vectors (left eigenvectors of $X$)\n",
    "    - $\\Sigma$: diagonal matrix of singular values (eigenvalues)\n",
    "    - $W$: d by d matrix of orthogonal unit vectors (right eigenvectors of $X$)\n",
    "- $W$ is our matrix of principal components\n",
    "- The explained variance is related to the diagonal entries of $\\Sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b11ca1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, WT = np.linalg.svd(X)\n",
    "\n",
    "print(\"WT:\\n\", WT)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efea36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Clustering\n",
    "\n",
    "- Another sub-problem in unsupervised learning is clustering\n",
    "- In clustering, observations are grouped together according to their similarity to other observations\n",
    "- The notion of similar typically depends on the *distance* between points accoring to some *metric*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c9617",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### K-means\n",
    "\n",
    "- A common clustering algorithm is called k-means clustering\n",
    "- In k-means clustering\n",
    "    - $k$ centroids (vectors) are chosen such that...\n",
    "    - The average distance from all points to their nearest centroid is minimized\n",
    "- Each observation is assigned to a single centroid\n",
    "- The set of observations assigned to the same centroid is called a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df0c01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Computing k-means\n",
    "\n",
    "- In order to apply k-means to a dataset, the following happens:\n",
    "    1. A number $k$ is fixed and a metric $\\rho$ is chosen\n",
    "    2. $k$ random vectors are proposed as centroids\n",
    "    3. Repeat the following until convergence:\n",
    "        1. Assign each observation to nearest current centroid\n",
    "        2. Move the centroid to the mean value of all its assigned observations\n",
    "- The process terminates with $k$ vectors that satisfy:\n",
    "    - All observations are closest to their assigned centroid that all other centroids\n",
    "    - All centroids are at the center of their assigned set of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396e2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can code up the proposed algorithm by hand..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fad75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def kmeans(k, X) -> np.ndarray:\n",
    "    # to get a proposal set of centroids, randomly\n",
    "    # sample from rows of X\n",
    "    inds = np.random.randint(0, X.shape[0], size=k)\n",
    "    centroids = X[inds, :]\n",
    "    while True:\n",
    "        labels = metrics.pairwise_distances_argmin(X, centroids)\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "        if np.max(np.abs(centroids - new_centroids)) < 1e-5:\n",
    "            return centroids, labels\n",
    "\n",
    "        centroids = new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5cbee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: made up data\n",
    "\n",
    "- We will use `sklearn.datasets.make_blobs` to generate clusters of data\n",
    "- We pick the number of samples, features, and clusters\n",
    "- We'll also visualize the clusters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dfd886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(42)\n",
    "k_true = 4\n",
    "X2, true_labels = datasets.make_blobs(n_samples=300, n_features=2, centers=k_true)\n",
    "true_centers = np.array([X2[true_labels==i].mean(axis=0) for i in range(k_true)])\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, centers, ax):\n",
    "    ax.scatter(X2[:, 0], X2[:, 1], c=color_cycle[labels])\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], marker=\"x\", c=\"k\", s=500);\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "plot_clusters(X2, true_labels, true_centers, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc6a2b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Let's test our kmeans function on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75275255",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kmeans(k_true, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9adff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88781f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884bc67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Notice that these centroids are not in the same order, but are approximately the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f35194",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers - true_centers[[2,0,3,1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2ce7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can verify that they group the points into the same clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "plot_clusters(X2, labels, centers, ax[0])\n",
    "plot_clusters(X2, true_labels, true_centers, ax[1])\n",
    "\n",
    "ax[0].set_title(\"From model\")\n",
    "ax[1].set_title(\"Truth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d71154",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Notes\n",
    "\n",
    "- The model discovered the clusters nearly perfectly (one observation was assigned to an incorrect cluster)\n",
    "- We have to choose the number of clusters\n",
    "    - This time we knew the \"correct\" answer... in practice we won't\n",
    "- This dataset was simulated specifically to have clusters of points around centroids\n",
    "- Kmeans can be slow for \"large\" data\n",
    "    - Each step requires computing distance between proposed centroid and *all* observations\n",
    "    - If either N or D is large, this can be costly"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
