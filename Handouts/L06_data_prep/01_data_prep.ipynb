{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58e2ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data Prep\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Pandas\n",
    "- Sklearn\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Be familiar with some common data prep tools: standardizing, scaling, feature encoding\n",
    "- Be able to construct a sklearn pipeline that does data preparation work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448d57e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "- It is often said that more than 90% of a data scientist's time is spent preparing data\n",
    "- That's likely an underestimate\n",
    "- In order to derive useful results from a model, you need to feed the model useful data\n",
    "- As the saying goes \"Garbage in, garbage out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab944b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Start early\n",
    "\n",
    "- The data preparation process begins before any data is actually collected!\n",
    "- Being part of experiment design or the data collection process is first best\n",
    "- When this is not possible, knowing as much as possible about data source will help\n",
    "    - Identify potential biases    \n",
    "    - Gain intuition on relationships between data\n",
    "    - Know what data *isn't* there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67571e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Types of data manipulation\n",
    "\n",
    "- Once you have some data and have decided what to model, you will likely need to prepare that data for the model\n",
    "- Some common transformations include:\n",
    "    - Standariding or scaling the data\n",
    "    - Feature encoding: one-hot-encoding, ordinal encoding, discritization\n",
    "    - Handling missing data: imputation, filtering, \n",
    "    - Feature engineering: polynomial features, other non-linear transforms\n",
    "- sklearn provides tools for all these types of pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8d80a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pipelines\n",
    "\n",
    "In sklearn there are two core parent classes\n",
    "\n",
    "1. `Transformers`: transform from $X$ to $\\hat{X}$\n",
    "    - `.fit(X)`: performs necessary calculations to do transformation (stores results)\n",
    "    - `.transform(X)`: does transform of $X$ to $\\hat{X}$\n",
    "1. `Estimators`: Given $X$ and $y$ data (or just $X$ for unsupervised) find model *parameters*\n",
    "    - `.fit(X, y)`: compute parameters of model\n",
    "    - `.predict(X)`: compute predicted $y$'s based on $X$'s that are passed in\n",
    "    \n",
    "> Note `.fit_transform(X)` is shorthand for first fitting, then transforming. Similarly `.fit_predict` will first fit and then generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34849d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### `sklearn.pipeline`\n",
    "\n",
    "- Many ML tasks require multiple steps of preprocessing before passing data to model\n",
    "- These are represented as transformers\n",
    "- A pipeline is a 0 or more transformers and then a single Estimator\n",
    "- Data is passed through transformers, in the order specified, then to estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42083ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### pipeline lifecycle\n",
    "\n",
    "1. Define the pipeline `model = sklearn.pipeline.make_pipeline([trans1, trans2, ..., transN, est])`\n",
    "2. Fit the model: `model.fit(X, y)`. Looks like this:\n",
    "```python\n",
    "X1 = trans1.fit_transform(X)\n",
    "X2 = trans2.fit_transform(X1)\n",
    "# ...\n",
    "XN = transN.fit_transform(XNm)\n",
    "est.fit(XN, y)\n",
    "```\n",
    "3. Generate predictions: `model.predict(x)`:\n",
    "```python\n",
    "x1 = trans1.transform(x)\n",
    "x2 = trans2.transform(x1)\n",
    "# ...\n",
    "xn = transN.transform(xNm)\n",
    "yhat = est.predict(xn)\n",
    "```\n",
    "\n",
    "> pipelines save you the hassle of calling `.fit` and `.transform` on all the transformers every time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde265b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46084182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, pipeline, linear_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc17fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some dummy data\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "y = np.array([0.1, -2.3, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1077a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and fit transformer\n",
    "trans1 = preprocessing.PolynomialFeatures(degree=3)\n",
    "trans1.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eca89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation to training data\n",
    "X1 = trans1.transform(X_train)\n",
    "print(X1.shape)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e698f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and fit linear model, using transformed data\n",
    "linreg = linear_model.LinearRegression()\n",
    "linreg.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b626c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training set\n",
    "linreg.predict(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661f9d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's try to evaluate on a test dataset\n",
    "X_test = np.array([[2, 3, 1], [-1, -1, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy to go wrong...\n",
    "linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e66e02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to transform first...\n",
    "X_test2 = trans1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441de0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then we can predict\n",
    "linreg.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd33e32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# easier to set up in a pipeline\n",
    "model = pipeline.make_pipeline(trans1, linreg)\n",
    "\n",
    "# single call to fit\n",
    "model.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f362f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76053c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single call to predict\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11015e",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "- Many machine learning algorithms require data to be scaled\n",
    "- Sometimes, the underlying math will even assume features `X` are distributed N(0, 1)\n",
    "- `sklearn.preprocessing.StandardScaler` is a routine to make each feature have mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8765645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"https://css-materials.s3.amazonaws.com/ML/linear_models_2/insurance_claims_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbers = df.select_dtypes([float, int])\n",
    "df_strings = df.select_dtypes([object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbers.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_numbers),\n",
    "    index=df_numbers.index,columns=df_numbers.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a636d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f3134",
   "metadata": {},
   "source": [
    "Notice mean and std are now (0,1) for all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d2015",
   "metadata": {},
   "source": [
    "Further Reference: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60326054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
