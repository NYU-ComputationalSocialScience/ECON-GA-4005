{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5faa8b01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow\n",
    "- Gradient descent\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the issues with gradient descent\n",
    "- Understand the concepts of momentum and adaptive learning rates\n",
    "- Understand the concept of epochs and batches\n",
    "- Be familiar with SGD variants like Adagrad, RMSprop, and ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe6d03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1390ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Local Minimum Problem\n",
    "\n",
    "- Gradient descent can be used to find a local minimum near the starting position\n",
    "- However, sometimes our starting position is close to a local minimum, but far from a global or better \"less-local\" minimum\n",
    "- For example, consider this objective function surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05512adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import (\n",
    "    plot_surf, grad_desc_tf, plot_path, plot_contour_path\n",
    ")\n",
    "\n",
    "def f2(x):\n",
    "    return  -tf.experimental.numpy.sinc(x[0])-2*tf.exp(-((x[0]-1)**2 + (x[1]-1)**2))\n",
    "\n",
    "ax = plot_surf(f2, lim=4, alpha=0.6)\n",
    "ax.view_init(10, 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79280bf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Gradient Descent Stuck\n",
    "\n",
    "- We will try to If we try to apply gradient descent to this surface \n",
    "- We'll choose a starting point of `[1.2, -1.9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522dd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = grad_desc_tf(f2, [1.2, -1.9], T=5000);\n",
    "ax = plot_path(f2, trace1, lim=4, alpha=0.4)\n",
    "ax.view_init(10, 30);\n",
    "plot_contour_path(f2, trace1, lim=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89924dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Need for Momentum\n",
    "\n",
    "- From contour plot above we see we move very little in `x2` dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652730a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1[0][\"x\"] - trace1[-1][\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6c55a",
   "metadata": {},
   "source": [
    "- Howevever we do move a little\n",
    "- Idea: could we continue to propogate movement in `x2` dimension to avoid getting stuck?\n",
    "- Alternative idea: could we \"magnify\" movements in x2 dimension where gradients are not as steep? (keep in mind -- will revisit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5fe8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Momentum\n",
    "\n",
    "- Let's explore continuing to propogate movement in \n",
    "- Intuition: build up momentum in each direction\n",
    "    - Will allow us to pass over \"flat\" spaces in dimensions with very small gradients\n",
    "    - Will also dampen movements in steep dimensions\n",
    "- Application: use exponentially decaying moving average to track momentum in each dimension\n",
    "    - Keep track of momentum on iteration $i$ using $v_i$\n",
    "    - Start with $v_0 = 0$\n",
    "    - Update rule for iteration $i$: $$\\begin{align*}v_{i+1} &= \\gamma v_i + \\alpha \\nabla f(x) \\\\ x_{i+1} &= x_i - v_{i+1}\\end{align*}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d207e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbf145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_tf_mom(f, x0, epsilon=1e-3, T=200, alpha=0.1, gamma=0.7):\n",
    "\n",
    "    trace = []\n",
    "    x = tf.Variable(x0)\n",
    "    v = tf.zeros(x.shape)  # NEW CODE\n",
    "    for i in range(T):\n",
    "        with tf.GradientTape() as tape:\n",
    "            fx = f(x)\n",
    "        dfdx = tape.gradient(fx, x)\n",
    "        vp = alpha*dfdx + gamma*v  # NEW CODE\n",
    "        xp = x - vp  # NEW CODE\n",
    "        err = max(abs(dfdx))\n",
    "        status = dict(\n",
    "            i=i, fx=fx.numpy(), dfdx=dfdx.numpy(), err=err.numpy(), x=x.numpy()\n",
    "        )\n",
    "        trace.append(status)\n",
    "        if err < epsilon and max(abs(vp)) < epsilon:\n",
    "            return trace\n",
    "\n",
    "        x = tf.Variable(xp)\n",
    "        v = vp  # NEW CODE\n",
    "\n",
    "    raise ValueError(\"No convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4b781",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Application\n",
    "\n",
    "- Let's try this momentum algorithm with our troublsome surface from above\n",
    "- Note we'll use the same learning rate and starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ee5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_mom = grad_desc_tf_mom(f2, [1.2, -1.9], T=1000);\n",
    "ax = plot_path(f2, trace_mom, lim=4, alpha=0.3)\n",
    "ax.view_init(10, 30);\n",
    "plot_contour_path(f2, trace_mom, lim=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4af770",
   "metadata": {},
   "source": [
    "- We no longer get stuck!\n",
    "- The small momentum built up in dimension `x2` allowed the algorithm to continue moving down that very slight slope until we find the global miminum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_mom[0][\"x\"] - trace_mom[-1][\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29fbb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Paper review\n",
    "\n",
    "- Gradient descent algorithms come in many variants\n",
    "- We've seen one example here that adds momementum\n",
    "- There are many others\n",
    "- There is an excellent review paper that describes the algorithms\n",
    "- **Reference**\n",
    "    - paper: https://arxiv.org/abs/1609.04747\n",
    "    - website: https://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec92542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
