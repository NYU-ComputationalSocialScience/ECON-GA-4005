{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6212ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; DeepMind\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- RL intro\n",
    "- Q learning\n",
    "- Q learning with continuous $\\mathcal{S}$\n",
    "- DQN intro\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand key technological breakthroughs in RL theory and application made by DeepMind team at Google\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 9-11, 16\n",
    "- [DeepMind website](https://deepmind.com/impact)\n",
    "- DeepMind papers:\n",
    "    - [\"Human-level control through deep reinforcement learning\"](https://www.nature.com/articles/nature14236) (DQN, 2013 & 2015)\n",
    "    - [\"Mastering the game of Go with deep neural networks and tree search\"](https://www.nature.com/articles/nature16961) (AlphaGo, 2016)\n",
    "    - [\"Mastering the game of Go without human knowledge\"](https://www.nature.com/articles/nature24270) (AlphaGo Zero, 2017)\n",
    "    - [\"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\"](https://arxiv.org/abs/1712.01815) (AlphaZero, 2017)\n",
    "    - [\"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\"](https://www.nature.com/articles/s41586-020-03051-4) (MuZero, 2020)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787abf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## DeepMind Overview\n",
    "\n",
    "- Company/Research group from London (100s of papers)\n",
    "- Have made incredible progress on RL algorithms since 2013\n",
    "- Acquired by Google in 2014\n",
    "- Specializes in Search algorithms and Deep Learning\n",
    "  - Combined them to create novel RL algorithms and solve difficult problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50946cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DQN and Atari (2013 & 2015)\n",
    "\n",
    "![img](./deepmind-atari-games.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957327f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "\n",
    "\n",
    "-   Paper that put DeepMind in the spotlight\n",
    "-   Constructed RL algorithm to play Atari games at super human levels using\n",
    "    only pixels from screen\n",
    "-   Opened the door for a flood of new research and academic focus on RL, mostly using DL techniques\n",
    "-   In coming years DeepMind would publish dozens of papers with variants of\n",
    "    the deep Q network introduced in this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d7bf6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Key Components of Approach\n",
    "\n",
    "- $s$ is pixels on Atari screen\n",
    "- $r$ is direction of change in game score (+1 if score went up, -1 if went down, 0 if no change)\n",
    "- $\\mathcal{A}$: nothing, 8 directions, button, 8 directions + button $\\Longrightarrow$ 18 choices\n",
    "- $Q: \\mathcal{S} \\rightarrow \\mathcal{R}^{|\\mathcal{A}|}$: convolutional neural network\n",
    "- Q-learning\n",
    "- Experience replay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67444c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Q-network\n",
    "\n",
    "\n",
    "\n",
    "![img](./dqn-atari.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d03b16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### DQN: Q-function Atari\n",
    "\n",
    "![Atari-Q-func.png](./Atari-Q-func.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d6e47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "![img](./atari-dqn-algo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa8c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Comments\n",
    "\n",
    "- Experience replay needed to reduce correlations and avoid falling into patterns\n",
    "    >  we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution (see below for details). \n",
    "- Updating $Q$ every $C$ steps also helps reduce correlations that lead to local maximua\n",
    "> we used an iterative update that adjusts the action-values ($Q$) towards target values that are only periodically updated, thereby reducing correlations with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810ea15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## AlphaGo (2016)\n",
    "\n",
    "- Algorithm that plays the board game Go at super-human levels\n",
    "- About Go\n",
    "    - Go originated in China over 3,000 years ago\n",
    "    - Widely considered to be amongst most difficult games for computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e118505",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Why is Go difficult?\n",
    "\n",
    "- Go is a game of perfect information (no randomness)\n",
    "- In theory, can be solved by recursive methods like value function iteration $\\Rightarrow$ $v^*(s)$\n",
    "- Would require searching across $b^d$ possible moves\n",
    "    - $b$: game's breadth -- number of local moves per position\n",
    "    - $d$: game's depth -- number or turns in game\n",
    "- For go $b \\approx 250$ and $d \\approx 150$\n",
    "- $b^d$ is a **HUGE** number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239cb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490909346529772655309577195498627564297521551249944956511154911718710525472171585646009788403733195227718357156513187851316791861042471890280751482410896345225310546445986192853894181098439730703830718994140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 250\n",
    "d = 150\n",
    "b**d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc561a7",
   "metadata": {},
   "source": [
    "- Trying to do an exhaustive search is not feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff02415",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Making Search Feasible\n",
    "\n",
    "- Two strategies for making search feasible\n",
    "    1. Lower depth of search by replacing $v^*(s)$ with an approximation $v(s)$ that predicts outcome of game, based on state $s$\n",
    "    2. Lower breadth of search by sampling from a policy $p(a | s)$ that is a probability distribution over $a$, given $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc445a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Key Algorithm Components\n",
    "\n",
    "- $s$: current structure of board\n",
    "- $a$: where to place next piece\n",
    "- $r$: 0 until terminal then +1 if victory and -1 if loss\n",
    "- $p_{\\sigma}(a |s)$: \n",
    "    - CNN based policy function\n",
    "    - Input $(s)$ uses depth to encode piece position and meaning according to rules of game\n",
    "    - Trained as supervised learning problem based on expert human moves from large database\n",
    "    - Objective of supervised learning is to predict human action, given state\n",
    "- $p_{\\pi}(a| s)$\n",
    "    - Shallow MLP based policy function\n",
    "    - Input $(s)$ is large vector encoding rules and current positions\n",
    "    - Less accurate that $p_{\\sigma}$, but very fast to evaluate\n",
    "    - Used when searching trees (see MCTS...)\n",
    "    - Trained same way as $p_{\\sigma}$\n",
    "- $p_p(a | s)$\n",
    "    - CNN based Q function\n",
    "    - Same input $s$ and network structure as $p_{\\sigma}$\n",
    "    - Weights are initialized to final $p_{\\sigma}$ weights \n",
    "    - Trained with self play using Q-learning\n",
    "- $v^p(s)$\n",
    "    - CNN based $v$ function\n",
    "    - Predicts outocme of game from state $s$, assuming policy $p$ is follwed by both players\n",
    "- Monte carlo tree search (MCTS)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f32d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### MCTS\n",
    "\n",
    "![mcts_alphago.png](./mcts_alphago.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd9f33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlphaGo Training\n",
    "\n",
    "- Final training of AlphaGo took incredible computational resources (thanks Google!)\n",
    "- Months of training time on 50 GPUs\n",
    "- This doesn't count trial and error training to figure out how to set everything up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c85f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlphaGo Results\n",
    "\n",
    "- Results are incredible\n",
    "- For first time, computer can beat a human grand master\n",
    "- [Famous](https://deepmind.com/alphago-korea) match against world champion Lee Sodol in 2016\n",
    "- Won 4 of 5 games in a series against Sodol\n",
    "- Played \"unconventional\" moves that human experts say they had never seen and taught them new concepts about Go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bb8d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## AlphaGo Zero (2017)\n",
    "\n",
    "- Building on success of AlphaGo, \n",
    "- AlphaGo Zero vastly out performs AlphaGo *entirely from self play*\n",
    "- Unlike AlphaGo, the Zero version doesn't ever see or use database of expert human moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c23e31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Key Algorithm Components\n",
    "\n",
    "- Will summarize differences from AlphaGo\n",
    "- $s$ raw board position and history\n",
    "    - AlphaGo used hand engineered features that encoded scoring systems and rule\n",
    "- Only uses one neural network (a $Q$ network) for policy network, value network, and rollout policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a31fdd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlphaGo Zero Training\n",
    "\n",
    "- Vastly simpler ML setup and training procedure\n",
    "- Much more efficient use of \n",
    "\n",
    "![alphago_zero.gif](./alphago_zero.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc564108",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlphaGo Zero Results\n",
    "\n",
    "- Not relying on expert human moves to bootstrap training allows AlphaGo Zero to learn unique playing style\n",
    "- Difficult for human players because many moves defy \"standard play\" amongst humans\n",
    "> Over the course of millions of AlphaGo vs AlphaGo games, the system progressively learned the game of Go from scratch, accumulating thousands of years of human knowledge during a period of just a few days. AlphaGo Zero also discovered new knowledge, developing unconventional strategies and creative new moves that echoed and surpassed the novel techniques it played in the games against Lee Sedol and Ke Jie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de312b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## AlphaZero (2017)\n",
    "\n",
    "- Why stop at Go?\n",
    "- Later in 2017, DeepMind team published a paper on AlphaZero that can play Go, Chess, and Shogi\n",
    "- Constructed a single RL algorithm and neural network structure (modulo necessary differences in input layer) that can play all three games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5abd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Key Algorithm Components\n",
    "\n",
    "- Will summarize differences from AlphaGo Zero\n",
    "- Does not take into account symmetry amongst $s$ \n",
    "    - many go board layouts are symmetric/identical, not true for other games\n",
    "    - Utilized in AlphaGo Zero to reduce size of state space\n",
    "- Self play opponent is always most recent agent\n",
    "    - Was \"best\" historical agent\n",
    "    - Removes need to evaluate most recent against current best\n",
    "- Hyperparameters (learning rates, network sizes etc) *fixed* for all games\n",
    "    - AlphaGo Zero used Bayesian techniques to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572825e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlphaZero Training\n",
    "\n",
    "![AlphaZero_training.png](AlphaZero_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4109d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## MuZero (2020)\n",
    "\n",
    "- Why stop at board games?\n",
    "- MuZero, introduced in 2020, can use the **same RL algorithm** to achieve super-human performance in Go, Shogi, Chess, and Atari games\n",
    "- Shows power of the RL, MCTS, and CNN tools used in previous DeepMind applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740e624",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Key Algorithm Components\n",
    "\n",
    "- Builds on AlphaZero, but adds a *learned model*\n",
    "- The learned model is composed of a hidden state, that is updated by the agent after each action\n",
    "- Model is compoosed of three key elements\n",
    "    - The value $v$: how good is the current position?\n",
    "    - The policy $p$: which action is the best to take?\n",
    "    - The reward $r$: how good was the last action?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b694aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is a Learned Model?\n",
    "\n",
    "- Inputs: history obervations $\\{o_t\\}_t$ from environment\n",
    "- Outputs:\n",
    "    - Representation function: $s^0 = h_\\theta(o_1, \\dots, o_t)$ (hidden state)\n",
    "    - Prediction function: $p^k, v^k = f_{\\theta}(s^k)$ (policy and value)\n",
    "    - Dynamics function $:r^k, s^k = g_{\\theta}(s^{k-1}, a^k)$ (reward and hidden state)\n",
    "\n",
    "![muzero_model.gif](muzero_model.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff145c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How to use a Model?\n",
    "\n",
    "- With model ($h_{\\theta}, f_{\\theta}, g_{\\theta}$) in hand, it can be used in RL training\n",
    "- Roll model forward to fill out tree in MCTS: results in MCTS values $\\nu$ and policies $\\pi$\n",
    "- Can select actions derived from MCTS policy: $a \\sim \\pi$\n",
    "\n",
    "![muzero_rollout.gif](muzero_rollout.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915684c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How to Learn a Model?\n",
    "\n",
    "- On each step, model is used to generate an action\n",
    "- Action is taken and \n",
    "\n",
    "![muzero_learning.gif](muzero_learning.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e81bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### MuZero Model Summary\n",
    "\n",
    "![muzero_algo_summary.png](muzero_algo_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f757c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### MuZero Outcomes\n",
    "\n",
    "- The key outocme from MuZero is the ability to apply RL techniques for optimizing decision making in environment with unknown dynamics\n",
    "- This opens the door to applications where \n",
    "    - Researchers don't know underlying dynamics\n",
    "    - Dynamics are formed from a \"messy\" system that would be difficult to encode as in classical RL environments\n",
    "- MuZero agent learns a hidden state $s_t$ that may be very different from observation $o_t$\n",
    "    - It is free to do this and will learn whatever $s_t$ is useful for prediction value, action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f0a41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary: Evolution of Algorithms\n",
    "\n",
    "![deepmind_evolution.png](deepmind_evolution.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
