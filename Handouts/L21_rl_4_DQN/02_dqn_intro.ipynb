{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61031815-c76b-450d-a565-29cca3a26b3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; Intro to Deep Q Networks\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow, keras\n",
    "- Reinforcement Learning -- Q learning with continuous state spaces\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Be able to use tensorflow to use a neural network to approximate $Q(s, a)$ for continuous spaces $\\mathcal{S}$\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 9-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2234ccd5-e651-477c-acc0-fb695e97e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4c042-217f-43e4-8856-df55a9e4b042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Review: CartPole\n",
    "\n",
    "- We previously studied the cart pole problem:\n",
    "    - Pole fastened to a cart, but can freely rotate\n",
    "    - Pole starts vertical, but with some angular velocity\n",
    "    - Goal: move cart left and right to keep pole vertical\n",
    "    - $\\mathcal{S} = \\{\\text{ cart position}, \\text{cart velocity}, \\text{ pole angle}, \\text{ pole angular velocity} \\} \\subset \\mathbb{R}^4$\n",
    "    - $\\mathcal{A}(s) = \\{\\text{ left, right }\\}\\; \\forall s$\n",
    "- Need $Q$ to generalize between observations from continuous $\\mathcal{S}$\n",
    "- Used complete polynomial to represent $Q$ and obtained about 120/200 tiem steps (random 30/120)\n",
    "- Need more flexible method for approximating $Q$..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a2b5f-34da-49a9-b99d-6a6f7e3e86a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Objective: DQN\n",
    "\n",
    "- The objective for this lecture will be to use a MLP for approximating $Q$\n",
    "- A few key concepts:\n",
    "    - Will represent $Q(s): \\mathcal{S} -> \\mathbb{R}^{|\\mathcal{A}|}$\n",
    "    - To support mini-batch training (and other reasons we'll learn about in another lecture ðŸ˜‰) we will use *experience replay*\n",
    "- Experience replay:\n",
    "    - Store $(s, a, r, s')$ transitions in a memory bank of fixed size\n",
    "    - As new transitions are added, \"forget\" oldest transitions if memory full\n",
    "    - When training, sample randomly from current memory bank to form batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abdd70-321f-4276-a634-936e78923671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DQN\n",
    "\n",
    "- Below we implement a Deep Q Network -- or a Q learning agent that uses a deep neural network for representing Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da94bee-8b2a-4faf-b87d-f79d27884c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(\n",
    "            self, \n",
    "            environment, \n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=keras.losses.mean_squared_error,\n",
    "            hidden_layer_sizes: List[int] = [24, 24],\n",
    "            batch_size: int = 64,\n",
    "            memory_size: int = 5_000,\n",
    "            epsilon: float=0.9, \n",
    "            beta: float=0.5\n",
    "        ):\n",
    "        # check that environment is what we think it is\n",
    "        self.env = environment\n",
    "        self.Ns = self.env.observation_space.shape[0]\n",
    "        \n",
    "        assert isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.Na = self.env.action_space.n\n",
    "        self.A_s = np.arange(self.Na)\n",
    "        \n",
    "        # set up Q function\n",
    "        self.Q = keras.Sequential(\n",
    "            [keras.layers.InputLayer((self.Ns,))] + \n",
    "            [keras.layers.Dense(n, activation=\"relu\") for n in hidden_layer_sizes] + \n",
    "            [keras.layers.Dense(\n",
    "                self.Na, activation=\"linear\",\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                    minval=-0.03, maxval=0.03\n",
    "                ),\n",
    "                bias_initializer=tf.keras.initializers.Constant(-0.2)\n",
    "            )]\n",
    "        )\n",
    "        self.Q.compile(loss=loss, optimizer=optimizer)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        \n",
    "        # set up memory\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # store hyper parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "\n",
    "    def get_greedy(self, s):\n",
    "        assert s.shape[0] == 1\n",
    "        Q_s = self.Q.predict(s)[0]\n",
    "        max_val = max(Q_s)\n",
    "        return random.choice(self.A_s[Q_s == max_val])\n",
    "    \n",
    "    def remember(self, s, a, r, sp, done):\n",
    "        self.memory.append((s, a, r, sp, done))\n",
    "    \n",
    "    def act(self, s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.get_greedy(s)\n",
    "    \n",
    "    def learn_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # not enough memory yet...\n",
    "            return\n",
    "                \n",
    "        # sample a batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        sarsd = list(zip(*batch))\n",
    "        \n",
    "        # reconstruct s, a, r, s' arrays\n",
    "        s = tf.concat(sarsd[0], axis=0)\n",
    "        a = np.row_stack(sarsd[1])[:, 0]\n",
    "        r = np.row_stack(sarsd[2])[:, 0]\n",
    "        sp = np.concatenate(sarsd[3])\n",
    "            \n",
    "        # compute temporal difference target using greedy policy\n",
    "        td_target = r + self.beta * tf.reduce_max(self.Q.predict(sp), axis=1)\n",
    "        \n",
    "        # apply one hot encoding for easy application of `a` below\n",
    "        a_hot = tf.one_hot(a, self.Na)\n",
    "        \n",
    "        # compute the loss between current Q(s, a) and the targets\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_s = self.Q(s)\n",
    "            Q_sa = tf.reduce_sum(Q_s * a_hot, axis=1)\n",
    "            l = self.loss(td_target, Q_sa)\n",
    "\n",
    "        # backprop -- compute and then allow optimizer to apply gradients\n",
    "        grads = tape.gradient(l, self.Q.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3de3c-6367-4085-b8ae-ea74824179eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Setup\n",
    "\n",
    "- Below we set random seeds, create env, optimizer, and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065ce782-2f36-4cdd-a880-f9d97d8c8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "agent = DQN(env, optimizer, epsilon=0.9, beta=0.8, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c144b4-15a0-4fd1-a4da-9be022a788dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.Q.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48255d04-c8ef-4fb0-96f6-61bc8596d31e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Training DQN\n",
    "\n",
    "- Below we have a routine for training our DQN\n",
    "- Notice that we need to make sure that the `s` array is [1, Ns] before handing to tensorflow\n",
    "    - Tensorflow expects first dimension to be for batch size and subsequent dimensions to be for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9672a52e-479f-4ac5-8203-b190896c940b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dqn, N_episodes=100):\n",
    "    for episode in range(N_episodes):\n",
    "        s = dqn.env.reset()[None, :]\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            a = dqn.act(s)\n",
    "            sp, r, done, _ = env.step(a)\n",
    "            sp = sp[None, :]\n",
    "            r = r if (not done or step >= 200) else -r  # penalize learner when fails\n",
    "\n",
    "            dqn.remember(s, a, r, sp, done)\n",
    "            if done:\n",
    "                # if episode % 100 == 0:\n",
    "                print(f\"episode: {episode}, steps: {step}\")\n",
    "                break\n",
    "\n",
    "            # step forward in time\n",
    "            s = sp\n",
    "            \n",
    "            # learn!\n",
    "            dqn.learn_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a392d65e-712f-498f-bb59-a9d15b5cddc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, steps: 15\n",
      "episode: 1, steps: 10\n",
      "episode: 2, steps: 9\n",
      "episode: 3, steps: 27\n",
      "episode: 4, steps: 20\n",
      "episode: 5, steps: 10\n",
      "episode: 6, steps: 15\n",
      "episode: 7, steps: 10\n",
      "episode: 8, steps: 9\n",
      "episode: 9, steps: 10\n",
      "episode: 10, steps: 9\n",
      "episode: 11, steps: 12\n",
      "episode: 12, steps: 8\n",
      "episode: 13, steps: 9\n",
      "episode: 14, steps: 9\n",
      "episode: 15, steps: 12\n",
      "episode: 16, steps: 27\n",
      "episode: 17, steps: 28\n",
      "episode: 18, steps: 22\n",
      "episode: 19, steps: 21\n",
      "episode: 20, steps: 30\n",
      "episode: 21, steps: 38\n",
      "episode: 22, steps: 85\n",
      "episode: 23, steps: 95\n",
      "episode: 24, steps: 196\n",
      "episode: 25, steps: 200\n",
      "episode: 26, steps: 200\n",
      "episode: 27, steps: 200\n",
      "episode: 28, steps: 200\n",
      "episode: 29, steps: 200\n",
      "episode: 30, steps: 198\n",
      "episode: 31, steps: 188\n",
      "episode: 32, steps: 200\n",
      "episode: 33, steps: 200\n",
      "episode: 34, steps: 200\n",
      "episode: 35, steps: 29\n",
      "episode: 36, steps: 200\n",
      "episode: 37, steps: 200\n",
      "episode: 38, steps: 117\n",
      "episode: 39, steps: 125\n",
      "episode: 40, steps: 200\n",
      "episode: 41, steps: 175\n",
      "episode: 42, steps: 116\n",
      "episode: 43, steps: 107\n",
      "episode: 44, steps: 157\n",
      "episode: 45, steps: 28\n",
      "episode: 46, steps: 175\n",
      "episode: 47, steps: 59\n",
      "episode: 48, steps: 88\n",
      "episode: 49, steps: 200\n"
     ]
    }
   ],
   "source": [
    "train(agent, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b510e-4aac-4cb7-8faa-111bb4b232e7",
   "metadata": {},
   "source": [
    "- After only 50 episodes our agent is regularly achieving the full 200 steps\n",
    "    - We chose $\\epsilon=0.9$, so we are forcing the agent to make random decisions 10% of the time\n",
    "    - If we evaluate in greedy mode, we would expect to see perfect scores more often\n",
    "- The added flexibility and generalization power we get from the MLP (relative to complete polynomial) is sufficient to succesfully complete this task!\n",
    "- We'll learn more about DQN and its recent exciting applications in another lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebbaf5-1a1b-49fd-af3f-bb2d2aafd8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
