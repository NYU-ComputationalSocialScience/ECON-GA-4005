{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a108e3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a5e2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "np.set_printoptions(linewidth=140, precision=4, suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5d72ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fabrizio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fabrizio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc435162",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text as data\n",
    "\n",
    "Language is constantly being stored online through academic papers, books, news articles, speeches, social media, product reviews, etc... This \"text data\", whether spoken or written, is one of the main sources of data accessible online.\n",
    "\n",
    "The ability to use and analyze this data would drastically increase the \"data available\" to data scientists, economists, and social scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0d84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What makes text difficult\n",
    "\n",
    "However, textual data comes with one main difficulty: The methods that we've discussed in this class (and in your other classes) are meant to deal with numerical data.\n",
    "\n",
    "In order to use text data (at scale) to gain insights into the world, we will need to find a way to transform the text data into numeric data. This transformation is difficult because language is a high dimensional object -- Words can have different meanings depending on how they are used and different combinations of words might map to the same meaning...\n",
    "\n",
    "This transformation and the subsequent analysis \"natural language processing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19769d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we will learn today\n",
    "\n",
    "* Tokenization\n",
    "* Binary presence\n",
    "* Bag of words\n",
    "\n",
    "Additionally, we will introduce some new Python libraries including,\n",
    "\n",
    "* `nltk` (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b1227",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are going to learn these using two \"fun\" examples as motivation. In both examples, we will attempt to associate text with its authors:\n",
    "\n",
    "1. [The Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers) were a set of 85 articles written by Alexander Hamilton, James Madison, and John Jay under the pseudonym Publius. We have convincing evidence for who wrote a particular article for ~70 of the articles but the authorship of the other 15 is disputed. An early paper on NLP used classification methods to determine the author of the disputed articles. We view this as a \"canonical\" application of NLP.\n",
    "2. Our other example will be in a similar vein. Dr. Sargent has written extensively about various topics with various coauthors. We will use the abstracts from some of these papers to determine whether we can determine who he coauthored a paper with given just the abstract. Classifying abstracts to various co-authors will be the simpler of the two tasks and we will use it as a chance to help us build the tools needed to tackle the Federalist example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54cc3f7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Federalist No. 1</td>\n",
       "      <td>General Introduction</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>To the People of the State of New York:\\n\\nAFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federalist No. 2</td>\n",
       "      <td>Concerning Dangers from Foreign Force and Infl...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nWHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federalist No. 3</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nIT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Federalist No. 4</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nMY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Federalist No. 5</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nQUE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                              title  \\\n",
       "1  Federalist No. 1                               General Introduction   \n",
       "2  Federalist No. 2  Concerning Dangers from Foreign Force and Infl...   \n",
       "3  Federalist No. 3  The Same Subject Continued (Concerning Dangers...   \n",
       "4  Federalist No. 4  The Same Subject Continued (Concerning Dangers...   \n",
       "5  Federalist No. 5  The Same Subject Continued (Concerning Dangers...   \n",
       "\n",
       "                         venue date    author  \\\n",
       "1  For the Independent Journal  NaN  Hamilton   \n",
       "2  For the Independent Journal  NaN       Jay   \n",
       "3  For the Independent Journal  NaN       Jay   \n",
       "4  For the Independent Journal  NaN       Jay   \n",
       "5  For the Independent Journal  NaN       Jay   \n",
       "\n",
       "                                                text  \n",
       "1  To the People of the State of New York:\\n\\nAFT...  \n",
       "2  To the People of the State of New York:\\n\\nWHE...  \n",
       "3  To the People of the State of New York:\\n\\nIT ...  \n",
       "4  To the People of the State of New York:\\n\\nMY ...  \n",
       "5  To the People of the State of New York:\\n\\nQUE...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We downloaded this DataFrame from the `corpus` R package. Big thanks to the\n",
    "# authors of this package for making this easy!\n",
    "federalist = pd.read_csv(\"federalist.csv\", index_col=0)\n",
    "\n",
    "federalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab5daea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coauthor</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cogley</td>\n",
       "      <td>For a vector autoregression with drifting coef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cogley</td>\n",
       "      <td>For postwar U.S. data, this paper uses Bayesia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cogley</td>\n",
       "      <td>Previous studies have interpreted the rise and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cogley</td>\n",
       "      <td>We estimate vector autoregressions with drifti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hansen</td>\n",
       "      <td>A representative consumer uses Bayes' law to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coauthor                                           abstract\n",
       "0   cogley  For a vector autoregression with drifting coef...\n",
       "1   cogley  For postwar U.S. data, this paper uses Bayesia...\n",
       "2   cogley  Previous studies have interpreted the rise and...\n",
       "3   cogley  We estimate vector autoregressions with drifti...\n",
       "4   hansen  A representative consumer uses Bayes' law to l..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in abstracts\n",
    "def read_abstract_file(fn):\n",
    "    author_abstract = []\n",
    "\n",
    "    with open(fn) as f:\n",
    "        foo = f.readlines()\n",
    "\n",
    "        for line in foo:\n",
    "            author, abstract = line.replace(\"\\n\", \"\").split(\",\", maxsplit=1)\n",
    "            author_abstract.append((author, abstract))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        author_abstract,\n",
    "        columns=[\"coauthor\", \"abstract\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "labeled_abstracts = read_abstract_file(\"../../Data/NLP/labeled_abstracts.csv\")\n",
    "unlabeled_abstracts = read_abstract_file(\"../../Data/NLP/unlabeled_abstracts.csv\")\n",
    "\n",
    "labeled_abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e5a0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skimming the abstracts\n",
    "\n",
    "Can we by hand pick some themes in the research Dr. Sargent has done with his coauthors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2194d46a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dispute about the size of the aggregate labor supply elasticity has been fortified by a contentious aggregation theory used by real business cycle theorists. The replacement of that aggregation theory with one more congenial to microeconomic observations opens possibilities for an accord about the aggregate labor supply elasticity. The new aggregation theory drops features to which empirical microeconomists objected and replaces them with life-cycle choices. Whether the new aggregation theory ultimately indicates a small or large macro labor supply elasticity will depend on how shocks and government institutions interact to put workers at interior solutions for career length.\n",
      "\n",
      "\n",
      "To generate big responses of unemployment to productivity changes, researchers have reconfigured matching models in various ways: by elevating the utility of leisure, by making wages sticky, by assuming alternating-offer wage bargaining, by introducing costly acquisition of credit, or by positing government mandated unemployment compensation and layoff costs. All of these redesigned matching models increase responses of unemployment to movements in productivity by diminishing the fundamental surplus fraction, an upper bound on the fraction of a job's output that the invisible hand can allocate to vacancy creation. This single common channel unites analyses of business cycle and welfare state dynamics.\n",
      "\n",
      "\n",
      "Ljungqvist and Sargent (1998, 2008) show that worse skill transition probabilities for workers who suffer involuntary layoffs (i.e., increases in turbulence) generate higher unemployment in a welfare state. den Haan, Haefke and Ramey (2005) challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss, then higher turbulence leads to lower unemployment within their matching model. We show (1) that there is no such brittleness of the positive turbulence-unemployment relationship in the matching model of Ljungqvist and Sargent (2007) even if we add such \"quit turbulence\", and (2) that if den Haan et al. had calibrated their productivity distribution to fit observed unemployment patterns that they miss, then they too would have found a positive turbulence-unemployment relationship in their model. Thus, we trace den Haan et al.'s finding to their assuming a narrower productivity distribution than Ljungqvist and Sargent had. Because den Haan et al. assume a distribution with such narrow support that it implies small returns to reallocating labor, even a small mobility cost shuts down voluntary separations. But that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects. When the parameterization is adjusted to fit historical observations on unemployment and layoff costs, a positive relationship between turbulence and unemployment reemerges.\n",
      "\n",
      "\n",
      "The same high labor supply elasticity that characterizes a representative family model with indivisible labor and employment lotteries also emerges without lotteries when self-insuring individuals choose interior solutions for their career lengths. Off corners, the more elastic is an earnings profile to accumulated working time, the longer is a workerʼs career. Negative (positive) unanticipated earnings shocks reduce (increase) the career length of a worker holding positive assets, while the effects are the opposite for a worker with negative assets. By inducing a worker to retire at an official retirement age, government provided social security can attenuate responses of career lengths to earnings profile slopes, earnings shocks, and taxes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = labeled_abstracts.query(\"coauthor == 'ljungqvist'\")[\"abstract\"]\n",
    "\n",
    "for (i, row) in foo.iteritems():\n",
    "    print(row)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c702c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The process by which we \"parse\" text and convert it into a smaller set of \"tokens\". Today, we'll convert the text by using words as tokens and will occasionally refer to this set of tokens as our \"vocabulary\".\n",
    "\n",
    "For example, consider the sentence,\n",
    "\n",
    ">The quick brown fox jumps over the lazy dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552d8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Unigram**\n",
    "\n",
    "One way we might \"tokenize\" this sentence would be to make each word its own token. This would give us the following tokens,\n",
    "\n",
    "1. The\n",
    "2. quick\n",
    "3. brown\n",
    "4. fox\n",
    "5. jumps\n",
    "6. over\n",
    "7. the\n",
    "8. lazy\n",
    "9. dog\n",
    "\n",
    "Our vocabulary would be 8 words with \"the\" appearing twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca32e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bigram**\n",
    "\n",
    "Another way we might tokenize this sentence is to make each pair of words its own token. This would give us the following tokens\n",
    "\n",
    "1. The quick\n",
    "2. quick brown\n",
    "3. brown fox\n",
    "4. fox jumps\n",
    "5. jumps over\n",
    "6. over the\n",
    "7. the lazy\n",
    "8. lazy dog\n",
    "\n",
    "Generically we refer to this type of tokenization as an _N-gram_. It is useful because it allows words like \"very\" to be more context dependent (\"very good\" vs \"very bad\") but comes at the cost of increasing the dimensionality of the words considered.\n",
    "\n",
    "In practice unigram will typically work well for analysis as it keeps the vocabulary dimension small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94074c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider the following paper abstracts,\n",
    "\n",
    ">Ljungqvist and Sargent (1998, 2008) show that worse skill transition probabilities for workers who suffer involuntary layoffs (i.e., increases in turbulence) generate higher unemployment in a welfare state. den Haan, Haefke and Ramey (2005) challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss, then higher turbulence leads to lower unemployment within their matching model. We show (1) that there is no such brittleness of the positive turbulence-unemployment relationship in the matching model of Ljungqvist and Sargent (2007) even if we add such \"quit turbulence\", and (2) that if den Haan et al. had calibrated their productivity distribution to fit observed unemployment patterns that they miss, then they too would have found a positive turbulence-unemployment relationship in their model. Thus, we trace den Haan et al.'s finding to their assuming a narrower productivity distribution than Ljungqvist and Sargent had. Because den Haan et al. assume a distribution with such narrow support that it implies small returns to reallocating labor, even a small mobility cost shuts down voluntary separations. But that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects. When the parameterization is adjusted to fit historical observations on unemployment and layoff costs, a positive relationship between turbulence and unemployment reemerges.\n",
    "\n",
    ">To understand trans-Atlantic employment experiences since World War II, we build an overlapping generations model with two types of workers (high school and college graduates) whose different skill acquisition technologies affect their career decisions. Search frictions affect short-run employment outcomes. The model focuses on labor supply responses near beginnings and ends of lives and on whether unemployment and early retirements are financed by personal savings or public benefit programs. Higher minimum wages in Europe explain why youth unemployment has risen more there than in the U.S. Turbulence, in the form of higher risks of human capital depreciation after involuntary job destructions, causes long-term unemployment in Europe, mostly among older workers, but leaves U.S. unemployment unaffected. The losses of skill interact with workers' subsequent decisions to invest in human capital in ways that generate the age-dependent increases in autocovariances of income shocks observed by Moffitt and Gottschalk (1995).\n",
    "\n",
    "These are both drawn from papers coauthored by Lars Ljungqvist and Thomas Sargent. They are on similar topics so they share certain common words which should allow us to find some way to associate the two abstracts.\n",
    "\n",
    "* What difficulties might there be in performing this association?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a10af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Capitalization**\n",
    "\n",
    "We typically normalize all text to be lower-case so that we don't generate unintended mismatches. For example, we would want to acknowledge that `turbulence` appears in both abstracts but is capitalized in the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0811b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Contractions**\n",
    "\n",
    "We will typically expand contractions into their full text meanings, i.e. \"don't\" -> \"do not\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b542870",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Numbers**\n",
    "\n",
    "Numbers without context in text don't always provide much useful information. For example, `(1)` and `(2)` would not help us understand what the abstract was about (and hence would not help us identify the author). On the other hand, there are contexts where the numbers are associated with years which might be helpful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3771b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Punctuation**\n",
    "\n",
    "In a \"fully-intelligent\" context, punctuation might help the computer break ideas into sentences, but, for what we do today, we will remove punctuation.\n",
    "\n",
    "One caveat to this is that there are many online contexts where subsets of punctuation marks can provide useful insights to what is being discussed -- For example, a tweet that contains `:-)` probably is a happy tweet whereas a tweet that contains `:-(` is likely a sad tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b9411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Plural, prefixes/suffixes, verb conjugation/tense...**\n",
    "\n",
    "Certain words that we might want to be associated with one another seem to appear in multiple ways. For example,\n",
    "\n",
    "* `risk`, `risks`\n",
    "* `employment`, `unemployment`\n",
    "* `had`, `have`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ab3d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stop words**\n",
    "\n",
    "For many applications, common words such as `the`, `and`, and other similar words to not provide much information about what is being discussed. We often will remove these words to cut down the number of words in our \"vocabulary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0592e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization\n",
    "\n",
    "We discuss a few ways to address some of the issues we raised above.\n",
    "\n",
    "We want to emphasize that different normalization methods are useful for different contexts. Blindly applying all of these normalization methods is likely to lead to a suboptimal result as you begin to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee527d01",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ljungqvist and Sargent (1998, 2008) show that worse skill transition probabilities for workers who suffer involuntary layoffs (i.e., increases in turbulence) generate higher unemployment in a welfare state. den Haan, Haefke and Ramey (2005) challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss, then higher turbulence leads to lower unemployment within their matching model. We show (1) that there is no such brittleness of the positive turbulence-unemployment relationship in the matching model of Ljungqvist and Sargent (2007) even if we add such \"quit turbulence\", and (2) that if den Haan et al. had calibrated their productivity distribution to fit observed unemployment patterns that they miss, then they too would have found a positive turbulence-unemployment relationship in their model. Thus, we trace den Haan et al.'s finding to their assuming a narrower productivity distribution than Ljungqvist and Sargent had. Because den Haan et al. assume a distribution with such narrow support that it implies small returns to reallocating labor, even a small mobility cost shuts down voluntary separations. But that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects. When the parameterization is adjusted to fit historical observations on unemployment and layoff costs, a positive relationship between turbulence and unemployment reemerges.\n"
     ]
    }
   ],
   "source": [
    "test_abstract = labeled_abstracts.loc[11, \"abstract\"]\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c122e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Make text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0e6b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ljungqvist and sargent (1998, 2008) show that worse skill transition probabilities for workers who suffer involuntary layoffs (i.e., increases in turbulence) generate higher unemployment in a welfare state. den haan, haefke and ramey (2005) challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss, then higher turbulence leads to lower unemployment within their matching model. we show (1) that there is no such brittleness of the positive turbulence-unemployment relationship in the matching model of ljungqvist and sargent (2007) even if we add such \"quit turbulence\", and (2) that if den haan et al. had calibrated their productivity distribution to fit observed unemployment patterns that they miss, then they too would have found a positive turbulence-unemployment relationship in their model. thus, we trace den haan et al.'s finding to their assuming a narrower productivity distribution than ljungqvist and sargent had. because den haan et al. assume a distribution with such narrow support that it implies small returns to reallocating labor, even a small mobility cost shuts down voluntary separations. but that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects. when the parameterization is adjusted to fit historical observations on unemployment and layoff costs, a positive relationship between turbulence and unemployment reemerges.\n"
     ]
    }
   ],
   "source": [
    "test_abstract = test_abstract.lower()\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f5243",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Remove numbers and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27804d2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ljungqvist and sargent   show that worse skill transition probabilities for workers who suffer involuntary layoffs ie increases in turbulence generate higher unemployment in a welfare state den haan haefke and ramey  challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss then higher turbulence leads to lower unemployment within their matching model we show  that there is no such brittleness of the positive turbulence unemployment relationship in the matching model of ljungqvist and sargent  even if we add such quit turbulence and  that if den haan et al had calibrated their productivity distribution to fit observed unemployment patterns that they miss then they too would have found a positive turbulence unemployment relationship in their model thus we trace den haan et als finding to their assuming a narrower productivity distribution than ljungqvist and sargent had because den haan et al assume a distribution with such narrow support that it implies small returns to reallocating labor even a small mobility cost shuts down voluntary separations but that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects when the parameterization is adjusted to fit historical observations on unemployment and layoff costs a positive relationship between turbulence and unemployment reemerges\n"
     ]
    }
   ],
   "source": [
    "test_abstract = test_abstract.replace(\"-\", \" \")\n",
    "\n",
    "# I hadn't used `translate` before! Cool!\n",
    "test_abstract = test_abstract.translate(\n",
    "    str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    ")\n",
    "\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03be112",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Lemmatization vs Stemming\n",
    "\n",
    "In order to compare versions of similar words, we can use one of two methods:\n",
    "\n",
    "* Lemmatization: Processes the sentence using vocabulary and converts words into their base form (the \"lemma\").\n",
    "* Stemming: A crude set of heuristics that chops off the ends of words in the hope of achieving the goal some of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5262f069",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ljungqvist and sargent   show that wors skill transit probabl for worker who suffer involuntari layoff ie increas in turbul gener higher unemploy in a welfar state den haan haefk and ramey  challeng thi find by show that if higher turbul mean that voluntari quit are also expos to even a tini risk of skill loss then higher turbul lead to lower unemploy within their match model we show  that there is no such brittl of the posit turbul unemploy relationship in the match model of ljungqvist and sargent  even if we add such quit turbul and  that if den haan et al had calibr their product distribut to fit observ unemploy pattern that they miss then they too would have found a posit turbul unemploy relationship in their model thu we trace den haan et al find to their assum a narrow product distribut than ljungqvist and sargent had becaus den haan et al assum a distribut with such narrow support that it impli small return to realloc labor even a small mobil cost shut down voluntari separ but that mean that the imposit of a small layoff cost in tranquil time ha counterfactu larg unemploy suppress effect when the parameter is adjust to fit histor observ on unemploy and layoff cost a posit relationship between turbul and unemploy reemerg\n"
     ]
    }
   ],
   "source": [
    "pstem = PorterStemmer()\n",
    "\n",
    "print(\" \".join(list(map(pstem.stem, test_abstract.split(\" \")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a25c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ljungqvist and sargent   show that worse skill transition probability for worker who suffer involuntary layoff ie increase in turbulence generate higher unemployment in a welfare state den haan haefke and ramey  challenge this finding by showing that if higher turbulence mean that voluntary quits are also exposed to even a tiny risk of skill loss then higher turbulence lead to lower unemployment within their matching model we show  that there is no such brittleness of the positive turbulence unemployment relationship in the matching model of ljungqvist and sargent  even if we add such quit turbulence and  that if den haan et al had calibrated their productivity distribution to fit observed unemployment pattern that they miss then they too would have found a positive turbulence unemployment relationship in their model thus we trace den haan et al finding to their assuming a narrower productivity distribution than ljungqvist and sargent had because den haan et al assume a distribution with such narrow support that it implies small return to reallocating labor even a small mobility cost shuts down voluntary separation but that mean that the imposition of a small layoff cost in tranquil time ha counterfactually large unemployment suppression effect when the parameterization is adjusted to fit historical observation on unemployment and layoff cost a positive relationship between turbulence and unemployment reemerges\n"
     ]
    }
   ],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "test_abstract = \" \".join(\n",
    "    map(lemm.lemmatize, test_abstract.split(\" \"))\n",
    ")\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d54b08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78da20c9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ljungqvist sargent   show worse skill transition probability worker suffer involuntary layoff ie increase turbulence generate higher unemployment welfare state den haan haefke ramey  challenge finding showing higher turbulence mean voluntary quits also exposed even tiny risk skill loss higher turbulence lead lower unemployment within matching model show  brittleness positive turbulence unemployment relationship matching model ljungqvist sargent  even add quit turbulence  den haan et al calibrated productivity distribution fit observed unemployment pattern miss would found positive turbulence unemployment relationship model thus trace den haan et al finding assuming narrower productivity distribution ljungqvist sargent den haan et al assume distribution narrow support implies small return reallocating labor even small mobility cost shuts voluntary separation mean imposition small layoff cost tranquil time ha counterfactually large unemployment suppression effect parameterization adjusted fit historical observation unemployment layoff cost positive relationship turbulence unemployment reemerges\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "\n",
    "    # Get list of stopwords\n",
    "    sw = stopwords.words(\"english\")\n",
    "    # Allocate space for all of our words\n",
    "    out = []\n",
    "\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in sw:\n",
    "            out.append(word)\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "print(remove_stopwords(test_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f10df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalizing all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc64ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coauthor</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cogley</td>\n",
       "      <td>For a vector autoregression with drifting coef...</td>\n",
       "      <td>vector autoregression drifting coefficient sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cogley</td>\n",
       "      <td>For postwar U.S. data, this paper uses Bayesia...</td>\n",
       "      <td>postwar u data paper us bayesian method accoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cogley</td>\n",
       "      <td>Previous studies have interpreted the rise and...</td>\n",
       "      <td>previous study interpreted rise fall u inflati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cogley</td>\n",
       "      <td>We estimate vector autoregressions with drifti...</td>\n",
       "      <td>estimate vector autoregressions drifting coeff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hansen</td>\n",
       "      <td>A representative consumer uses Bayes' law to l...</td>\n",
       "      <td>representative consumer us bayes law learn par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coauthor                                           abstract  \\\n",
       "0   cogley  For a vector autoregression with drifting coef...   \n",
       "1   cogley  For postwar U.S. data, this paper uses Bayesia...   \n",
       "2   cogley  Previous studies have interpreted the rise and...   \n",
       "3   cogley  We estimate vector autoregressions with drifti...   \n",
       "4   hansen  A representative consumer uses Bayes' law to l...   \n",
       "\n",
       "                                      clean_abstract  \n",
       "0  vector autoregression drifting coefficient sto...  \n",
       "1  postwar u data paper us bayesian method accoun...  \n",
       "2  previous study interpreted rise fall u inflati...  \n",
       "3  estimate vector autoregressions drifting coeff...  \n",
       "4  representative consumer us bayes law learn par...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalizer(text):\n",
    "    # First, set all text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Now remove punctuation and numbers\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.translate(\n",
    "        str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    "    )\n",
    "\n",
    "    # Remove anywhere with more than two consecutive whitespaces\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemm = WordNetLemmatizer()\n",
    "    text = \" \".join(\n",
    "        map(lemm.lemmatize, text.split(\" \"))\n",
    "    )\n",
    "\n",
    "    # Remove stopwords\n",
    "    sw = stopwords.words(\"english\")\n",
    "    out = []\n",
    "\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in sw:\n",
    "            out.append(word)\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "labeled_abstracts[\"clean_abstract\"] = labeled_abstracts[\"abstract\"].map(normalizer)\n",
    "unlabeled_abstracts[\"clean_abstract\"] = unlabeled_abstracts[\"abstract\"].map(normalizer)\n",
    "labeled_abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0391be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary presence\n",
    "\n",
    "Once we've tokenized our data, we must choose a way to convert our tokens into numeric data. The simplest way to do this is simply categorize whether or not a value was present in the text or not.\n",
    "\n",
    "Imagine that our vocabulary is `[\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]` then the sentence \"The quick brown fox jumps over the lazy brown dog\" would get converted to numeric data by placing a 1 for each vocabulary word that matches any token in the tokenized data. This would result in `[1, 1, 1, 0, 1, 0]`\n",
    "\n",
    "Even though the word `\"brown\"` showed up multiple times, it just gets labeled as a 1 because we are just testing for the presence of certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cbe98e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_presence(vocabulary, text):\n",
    "    # Allocate memory\n",
    "    out = np.zeros(len(vocabulary), dtype=int)\n",
    "\n",
    "    # Check whether the word is in our text\n",
    "    for (iv, v) in enumerate(vocabulary):\n",
    "        out[iv] = 1 if v in text else 0\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build our test vocabulary\n",
    "vocabulary = [\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]\n",
    "text = \"the quick brown fox jumps over the lazy brown dog\"\n",
    "\n",
    "# Check whether this lines up with expected result\n",
    "binary_presence(vocabulary, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276ba4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing vocabulary\n",
    "\n",
    "Binary presence seems easy enough to implement! However, we are left with one remaining detail... How do we choose a vocabulary?\n",
    "\n",
    "One choice is to take _ALL_ words possible, but that will leave us with many words that might add a limited amount of information.\n",
    "\n",
    "The way that we will do it in this example is to find all of the words that show up in at least 3 abstracts. We have at least 4 abstracts from a paper written with each coauthor so words that are used frequently in abstracts with particular coauthors should show if we count words that are in 3 abstracts.\n",
    "\n",
    "Ultimately, the way that you build your vocabulary will be up to you (and be usage dependent!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ae83e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Start with all words\n",
    "all_words = list(\n",
    "    set(\n",
    "        labeled_abstracts[\"clean_abstract\"].map(\n",
    "            lambda x: x.split(\" \")\n",
    "        ).sum()\n",
    "    )\n",
    ")\n",
    "all_words.sort()\n",
    "\n",
    "# Count the number of times each word shows up\n",
    "# in an abstract\n",
    "out = np.zeros(len(all_words), dtype=bool)\n",
    "for (iw, word) in enumerate(all_words):\n",
    "    count_abstracts = (\n",
    "        labeled_abstracts[\"clean_abstract\"]\n",
    "        .str\n",
    "        .contains(word)\n",
    "        .sum()\n",
    "    )\n",
    "    out[iw] = count_abstracts >= 3\n",
    "    \n",
    "abstract_vocabulary = [\n",
    "    word for (iw, word) in enumerate(all_words) if out[iw]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9a12d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayesian classification\n",
    "\n",
    "Our favorite law (Bayes law) shows up again! We want to classify whether an abstract was written with a particular coauthor given which words from our vocabulary show up in the abstract.\n",
    "\n",
    "Let $y_j$ denote each coauthor (Tim Cogley, Lars Hansen, or Lars Ljungqvist) and $x_i$ denote whether or not each word from the vocabulary appeared in the abstract, then:\n",
    "\n",
    "$$P(y_j | x_1, x_2, \\dots, x_n) = \\frac{P(y) P(x_1, x_2, \\dots, x_n | y_j)}{P(x_1, x_2, \\dots, x_n)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf372a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why \"naive\"?**\n",
    "\n",
    "We refer to this classification method as \"naive\" because we assume that, given the coauthor that the probability of observing each word is independent to the others.\n",
    "\n",
    "$$P(x_1, x_2, \\dots, x_n | y_j) = P(x_1 | y_j) P(x_2 | y_j) \\dots P(x_n | y_j)$$\n",
    "\n",
    "There is no basis for this assumption and is simply made to simplify the mathematics. In fact, we could probably argue that this is a bad assumption for our data, but, in spite of this, it will prove to be successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d3c29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Parameter estimation**\n",
    "\n",
    "We will assume that an abstract is equally likely to have been written by each coauthor, i.e., $P(y_j) = P(y_k) = \\frac{1}{3}$\n",
    "\n",
    "The probabilities $P(x_i | y_j)$ are computed by using something similar to a beta-binomial conjugate prior (but allows for more than 2 classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4dce09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e4d1e6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nobs = labeled_abstracts.shape[0]\n",
    "nwords = len(abstract_vocabulary)\n",
    "\n",
    "coauthors = {\n",
    "    \"cogley\": 0,\n",
    "    \"hansen\": 1,\n",
    "    \"ljungqvist\": 2\n",
    "}\n",
    "\n",
    "X = np.zeros((nobs, nwords), dtype=int)\n",
    "y = np.zeros(nobs, dtype=int)\n",
    "\n",
    "for i in range(nobs):\n",
    "    y[i] = coauthors[labeled_abstracts.at[i, \"coauthor\"]]\n",
    "\n",
    "    for (j, word) in enumerate(abstract_vocabulary):\n",
    "        X[i, j] = word in labeled_abstracts.at[i, \"clean_abstract\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db375c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Estimate using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9704f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(fit_prior=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB(fit_prior=False)\n",
    "\n",
    "mnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92760a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Can we predict who the remaining abstracts were written with?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf82754",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We estimate a Bayesian vector autoregression for the U.K. with drifting coefficients and stochastic volatilities. We use it to characterize posterior densities for several objects that are useful for designing and evaluating monetary policy, including local approximations to the mean, persistence, and volatility of inflation. We present diverse sources of uncertainty that impinge on the posterior predictive density for inflation, including model uncertainty, policy drift, structural shifts and other shocks. We use a recently developed minimum entropy method to bring outside information to bear on inflation forecasts. We compare our predictive densities with the Bank of England's fan charts.\n",
      "\n",
      "\n",
      "An ambiguity averse decision maker evaluates plans under a restricted family of structured models and unstructured alternatives that are statistically close to them. Structured models include parametric models in which parameter values vary over time in ways that the decision maker cannot describe probabilistically. Because he suspects that all parametric models are misspecified, the decision maker also evaluates plans under alternative probability distributions with much less structure.\n",
      "\n",
      "\n",
      "This paper studies alternative ways of representing uncertainty about a law of motion in a version of a classic macroeconomic targetting problem of Milton Friedman (1953). We study both “unstructured uncertainty” – ignorance of the conditional distribution of the target next period as a function of states and controls – and more “structured uncertainty” – ignorance of the probability distribution of a response coefficient in an otherwise fully trusted specification of the conditional distribution of next period׳s target. We study whether and how different uncertainties affect Friedman׳s advice to be cautious in using a quantitative model to fine tune macroeconomic outcomes.\n",
      "\n",
      "\n",
      "The fundamental surplus isolates parameters that determine how sensitively unemployment responds to productivity shocks in the matching models of Christiano et al. (2016 and this issue) under either Nash bargaining or alternating-offer bargaining. Those models thus join a collection of models in which diverse forces are intermediated through the fundamental surplus.\n",
      "\n",
      "\n",
      "To understand trans-Atlantic employment experiences since World War II, we build an overlapping generations model with two types of workers (high school and college graduates) whose different skill acquisition technologies affect their career decisions. Search frictions affect short-run employment outcomes. The model focuses on labor supply responses near beginnings and ends of lives and on whether unemployment and early retirements are financed by personal savings or public benefit programs. Higher minimum wages in Europe explain why youth unemployment has risen more there than in the U.S. Turbulence, in the form of higher risks of human capital depreciation after involuntary job destructions, causes long-term unemployment in Europe, mostly among older workers, but leaves U.S. unemployment unaffected. The losses of skill interact with workers' subsequent decisions to invest in human capital in ways that generate the age-dependent increases in autocovariances of income shocks observed by Moffitt and Gottschalk (1995).\n",
      "\n",
      "\n",
      "We study how a concern for robustness modifies a policymaker's incentive to experiment. A policymaker has a prior over two submodels of inflation-unemployment dynamics. One submodel implies an exploitable trade-off, the other does not. Bayes' law gives the policymaker an incentive to experiment. The policymaker fears that both submodels and his prior probability distribution over them are misspecified. We compute decision rules that are robust to misspecifications of each submodel and of the prior distribution over submodels. We compare robust rules to ones that Cogley, Colacito, and Sargent (2007) computed assuming that the models and the prior distribution are correctly specified. We explain how the policymaker's desires to protect against misspecifications of the submodels, on the one hand, and misspecifications of the prior over them, on the other, have different effects on the decision rule.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, ab) in unlabeled_abstracts[\"abstract\"].iteritems():\n",
    "    print(ab)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0bc89fc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "noos = unlabeled_abstracts.shape[0]\n",
    "\n",
    "X_oos = np.zeros((noos, nwords), dtype=int)\n",
    "y_oos = np.zeros(noos, dtype=int)\n",
    "\n",
    "for i in range(noos):\n",
    "    y_oos[i] = coauthors[unlabeled_abstracts.at[i, \"coauthor\"]]\n",
    "\n",
    "    for (j, word) in enumerate(abstract_vocabulary):\n",
    "        X_oos[i, j] = word in unlabeled_abstracts.at[i, \"clean_abstract\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd7163da",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(X_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8aaba53",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 2, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f2e6146",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9974, 0.0025, 0.0001],\n",
       "       [0.0002, 0.9962, 0.0036],\n",
       "       [0.0023, 0.9939, 0.0037],\n",
       "       [0.0216, 0.0358, 0.9426],\n",
       "       [0.0029, 0.0021, 0.995 ],\n",
       "       [0.0766, 0.2118, 0.7116]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict_proba(X_oos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796687a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "[Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) will be similar to binary presence, but, rather than simple track whether a word shows up in the text, we will count the number of times it shows up.\n",
    "\n",
    "The \"implicit assumption\" in bag of words models is that words are drawn from an urn with certain probabilities and that order is unimportant.\n",
    "\n",
    "We revisit our example from before:\n",
    "\n",
    "Imagine that our vocabulary is `[\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]` then the sentence \"The quick brown fox jumps over the lazy brown dog\" would get converted to numeric data by counting the number of times each vocabulary word appears in the tokenized data. This would result in `[1, 2, 1, 0, 1, 0]`\n",
    "\n",
    "Before, we placed a 1 in the vector element that corresponded to brown, but since we are now counting occurrences, we place a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c41e243a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(vocabulary, text):\n",
    "    # Check whether the word is in our text\n",
    "    dcount = {word: 0 for word in vocabulary}\n",
    "    for word in text.split(\" \"):\n",
    "        if word in vocabulary:\n",
    "            dcount[word] += 1\n",
    "\n",
    "    out = np.array([dcount[word] for word in vocabulary])\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build our test vocabulary\n",
    "vocabulary = [\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]\n",
    "text = \"the quick brown fox jumps over the lazy brown dog\"\n",
    "\n",
    "# Check whether this lines up with expected result\n",
    "bag_of_words(vocabulary, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f692e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing vocabulary\n",
    "\n",
    "We must again consider how to choose our vocabulary. The Federalist papers were all written on a similar topic, so many of the words will be shared across the documents (one of the reasons why binary presence will not perform particularly well in this case).\n",
    "\n",
    "Rather than target the \"topic\" covered in each document, we will attempt to extract information on the writing style by tracking the way certain words are used. In the original paper, Mosteller and Wallace choose 30 (relatively common) words that they think can proxy for each of the individual authors' writing styles. They call these words \"markers\" and they highlight that Hamilton and Madison use words differently. For example, In the 14 essays written by Madison, the word _while_ never occurs but _whilst_ occurs in 8 of them. Similarly, _while_ occurs in 15 of the 48 Hamilton essays, but _whist_ never occurs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9942fa4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "federalist_vocabulary = [\n",
    "    \"upon\", \"also\", \"an\", \"by\", \"of\", \"on\", \"there\", \"this\", \"to\",\n",
    "    \"although\", \"both\", \"enough\", \"while\", \"whilst\", \"always\", \"though\",\n",
    "    \"commonly\", \"consequently\", \"considerable\", \"according\", \"apt\",\n",
    "    \"direction\", \"innovation\", \"language\", \"vigor\", \"kind\", \"matter\",\n",
    "    \"particularly\", \"probability\", \"work\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fb00f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalizing the data\n",
    "\n",
    "Given that our goals are slightly different this time, our normalization procedure will look slightly different.\n",
    "\n",
    "In particular, we will _not_ want to remove stopwords because many of the vocabularly words that we're using to identify the styles are stopwords.\n",
    "\n",
    "We will also directly replace some of the various tenses of words rather than use a lemmatizer or stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4e7a624",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Federalist No. 1</td>\n",
       "      <td>General Introduction</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>To the People of the State of New York:\\n\\nAFT...</td>\n",
       "      <td>to the people of the state of new york after a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federalist No. 2</td>\n",
       "      <td>Concerning Dangers from Foreign Force and Infl...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nWHE...</td>\n",
       "      <td>to the people of the state of new york when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federalist No. 3</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nIT ...</td>\n",
       "      <td>to the people of the state of new york it is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Federalist No. 4</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nMY ...</td>\n",
       "      <td>to the people of the state of new york my last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Federalist No. 5</td>\n",
       "      <td>The Same Subject Continued (Concerning Dangers...</td>\n",
       "      <td>For the Independent Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jay</td>\n",
       "      <td>To the People of the State of New York:\\n\\nQUE...</td>\n",
       "      <td>to the people of the state of new york queen a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                              title  \\\n",
       "1  Federalist No. 1                               General Introduction   \n",
       "2  Federalist No. 2  Concerning Dangers from Foreign Force and Infl...   \n",
       "3  Federalist No. 3  The Same Subject Continued (Concerning Dangers...   \n",
       "4  Federalist No. 4  The Same Subject Continued (Concerning Dangers...   \n",
       "5  Federalist No. 5  The Same Subject Continued (Concerning Dangers...   \n",
       "\n",
       "                         venue date    author  \\\n",
       "1  For the Independent Journal  NaN  Hamilton   \n",
       "2  For the Independent Journal  NaN       Jay   \n",
       "3  For the Independent Journal  NaN       Jay   \n",
       "4  For the Independent Journal  NaN       Jay   \n",
       "5  For the Independent Journal  NaN       Jay   \n",
       "\n",
       "                                                text  \\\n",
       "1  To the People of the State of New York:\\n\\nAFT...   \n",
       "2  To the People of the State of New York:\\n\\nWHE...   \n",
       "3  To the People of the State of New York:\\n\\nIT ...   \n",
       "4  To the People of the State of New York:\\n\\nMY ...   \n",
       "5  To the People of the State of New York:\\n\\nQUE...   \n",
       "\n",
       "                                          clean_text  \n",
       "1  to the people of the state of new york after a...  \n",
       "2  to the people of the state of new york when th...  \n",
       "3  to the people of the state of new york it is n...  \n",
       "4  to the people of the state of new york my last...  \n",
       "5  to the people of the state of new york queen a...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalizer_federalist(text):\n",
    "    # First, set all text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Now remove punctuation and numbers\n",
    "    text = text.translate(\n",
    "        str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    "    )\n",
    "\n",
    "    # Remove anywhere with more than two consecutive whitespaces\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    # Replace certain words\n",
    "    replacer = [\n",
    "        (\"matters\", \"matter\"),\n",
    "        (\"considerably\", \"considerable\"),\n",
    "        (\"innovations\", \"innovation\"),\n",
    "        (\"vigorous\", \"vigor\"),\n",
    "        (\"works\", \"work\")\n",
    "    ]\n",
    "    for (_w, _r) in replacer:\n",
    "        text.replace(_w, _r)\n",
    "\n",
    "    return text\n",
    "\n",
    "federalist[\"clean_text\"] = federalist[\"text\"].map(\n",
    "    normalizer_federalist\n",
    ")\n",
    "\n",
    "federalist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81f258",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Additionally, rather than keep the counts directly, we will convert the counts to a rate per 100,000 words and round to the nearest integer. We do this to make sure that the documents are comparable in spite of differing in length. We convert to integers because our naive Bayes classifier assumes that the inputs are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acff10d7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "federalist[\"nwords\"] = federalist[\"clean_text\"].str.split(\" \").map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c804a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26956fb2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Separate into known and unknown authors\n",
    "known_federalist = (\n",
    "    federalist\n",
    "    .loc[~federalist[\"author\"].isna(), :]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "unknown_federalist = (\n",
    "    federalist\n",
    "    .loc[federalist[\"author\"].isna(), :]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "nobs = known_federalist.shape[0]\n",
    "nvocab = len(federalist_vocabulary)\n",
    "\n",
    "publius = {\n",
    "    \"Hamilton\": 0,\n",
    "    \"Jay\": 1,\n",
    "    \"Madison\": 2\n",
    "}\n",
    "\n",
    "X = np.zeros((nobs, nvocab), dtype=int)\n",
    "y = np.zeros(nobs, dtype=int)\n",
    "\n",
    "for i in range(nobs):\n",
    "    y[i] = publius[known_federalist.at[i, \"author\"]]\n",
    "    X[i, :] = bag_of_words(\n",
    "        federalist_vocabulary,\n",
    "        known_federalist.at[i, \"clean_text\"]\n",
    "    )\n",
    "\n",
    "# Convert to rates\n",
    "nwords = known_federalist.loc[:, \"nwords\"].to_numpy()\n",
    "X = np.round(100_000 * X/nwords[:, None]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406dd9d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Estimate with sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ac8192e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(fit_prior=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB(fit_prior=False)\n",
    "\n",
    "mnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3b811",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Predict unknown authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ecea840",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "noos = unknown_federalist.shape[0]\n",
    "\n",
    "X_oos = np.zeros((noos, nvocab), dtype=int)\n",
    "\n",
    "for i in range(noos):\n",
    "    X_oos[i, :] = bag_of_words(\n",
    "        federalist_vocabulary,\n",
    "        unknown_federalist.at[i, \"clean_text\"]\n",
    "    )\n",
    "\n",
    "# Convert to rates\n",
    "nwords = unknown_federalist.loc[:, \"nwords\"].to_numpy()\n",
    "X_oos = np.round(100_000 * X_oos/nwords[:, None]).astype(int)\n",
    "\n",
    "probs_oos = mnb.predict_proba(X_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0603acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_h = known_federalist.query(\"author == 'Hamilton'\").index\n",
    "wpht_h = X[idx_h, :].mean(axis=0)\n",
    "\n",
    "idx_m = known_federalist.query(\"author == 'Madison'\").index\n",
    "wpht_m = X[idx_m, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a49467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 500.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAR+0lEQVR4nO3dW6xcZf3G8e/zr3gInkAKadrqRtMY8RA0DZpgjIkHKhiLRkxJJDXR4AUkkJho4UY0adIYNXqhJnhIajyQJqA0cqEEIcoNuIsolIo0WqHStEXioTca4Pe/mFUdyj7M7swwM2+/n4TMWu9aM+v39mWeWfudmTWpKiRJbfm/SRcgSRo9w12SGmS4S1KDDHdJapDhLkkNesGkCwA466yzam5ubtJl/NcDf/3HktvfvPYVI92vf99RG3VfJE2PPXv2PFFVqxfaNhXhPjc3x/z8/KTL+K+5bbctuX1+xyUj3a9/31EbdV8kTY8kf1lsm9MyktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjgcE+yKslvk/ysWz8zye1JHuluz+jb97ok+5M8nOSicRQuSVrcSs7crwH29a1vA+6oqg3AHd06Sc4DtgBvBDYB30yyajTlSpIGMVC4J1kHXAJ8p695M7CzW94JXNrXflNV/buq/gzsBy4YSbWSpIEMeub+NeCzwDN9bedU1SGA7vbsrn0t8Fjffge7tmdJcmWS+STzR48eXWndkqQlLBvuST4IHKmqPQM+ZhZoq+c0VN1YVRurauPq1Qv+vqsk6SQN8gPZFwIfSnIx8GLg5Ul+ABxOsqaqDiVZAxzp9j8IrO+7/zrg8VEWLUla2rJn7lV1XVWtq6o5em+U/rKqPg7sBrZ2u20Fbu2WdwNbkrwoybnABuDekVcuSVrUIGfui9kB7ErySeBR4DKAqtqbZBfwEPAUcFVVPT10pZKkga0o3KvqLuCubvlvwHsW2W87sH3I2iRJJ8lvqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNMw3VDUGc9tuW3L7gR2XPE+VSJplTYT7qRiIp2KfJQ3OaRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBi0b7knWJ7kzyb4ke5Nc07WfmeT2JI90t2f03ee6JPuTPJzkonF2QJL0XIOcuT8FfKaq3gC8A7gqyXnANuCOqtoA3NGt023bArwR2AR8M8mqcRQvSVrYsuFeVYeq6r5u+V/APmAtsBnY2e22E7i0W94M3FRV/66qPwP7gQtGXLckaQkrmnNPMge8FbgHOKeqDkHvBQA4u9ttLfBY390Odm0nPtaVSeaTzB89evQkSpckLWbgcE/yUuBm4Nqq+udSuy7QVs9pqLqxqjZW1cbVq1cPWoYkaQADhXuS0+gF+w+r6pau+XCSNd32NcCRrv0gsL7v7uuAx0dTriRpEIN8WibAd4F9VfXVvk27ga3d8lbg1r72LUlelORcYANw7+hKliQt5wUD7HMhcAXwQJL7u7brgR3AriSfBB4FLgOoqr1JdgEP0fukzVVV9fSoC5ckLW7ZcK+qu1l4Hh3gPYvcZzuwfYi6JElD8BuqktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDfJLTNIpb27bbUtuP7DjkuepEmkwnrlLUoMMd0lqkNMykmaW02WL88xdkhpkuEtSg5yW0SnNP+vVKsNdY2NwSpPjtIwkNcgzd2mKLffXD/gXkBZmuEtq3qk4RXhKhfupOMCzwHF5fvnvPRrT/u94SoW7Ztu0P5la47/3bDPcpREyEDUtxhbuSTYBXwdWAd+pqh3jOpYkX1j0bGMJ9ySrgG8A7wMOAr9JsruqHhrH8SS1ZVIvVOM47qT6Mq4z9wuA/VX1J4AkNwGbAcNdOkX5sc7nV6pq9A+afBTYVFWf6tavAN5eVVf37XMlcGW3+nrg4RGWcBbwxAgfb5Lsy3SyL9PpVOvLa6pq9UIbxnXmngXanvUqUlU3AjeO5eDJfFVtHMdjP9/sy3SyL9PJvvzPuC4/cBBY37e+Dnh8TMeSJJ1gXOH+G2BDknOTvBDYAuwe07EkSScYy7RMVT2V5Grg5/Q+Cvm9qto7jmMtYizTPRNiX6aTfZlO9qUzljdUJUmT5SV/JalBhrskNaipcE+yKcnDSfYn2TbpeoaR5ECSB5Lcn2R+0vWsVJLvJTmS5MG+tjOT3J7kke72jEnWOKhF+nJDkr9243N/kosnWeMgkqxPcmeSfUn2Jrmma5+5cVmiL7M4Li9Ocm+S33V9+ULXPtS4NDPn3l3y4I/0XfIAuHxWL3mQ5ACwsapm8gsZSd4FHAO+X1Vv6tq+BDxZVTu6F98zqupzk6xzEIv05QbgWFV9eZK1rUSSNcCaqrovycuAPcClwCeYsXFZoi8fY/bGJcDpVXUsyWnA3cA1wEcYYlxaOnP/7yUPquo/wPFLHmgCqupXwJMnNG8GdnbLO+k9GafeIn2ZOVV1qKru65b/BewD1jKD47JEX2ZO9RzrVk/r/iuGHJeWwn0t8Fjf+kFmdLA7BfwiyZ7uUg0tOKeqDkHvyQmcPeF6hnV1kt930zZTP5XRL8kc8FbgHmZ8XE7oC8zguCRZleR+4Ahwe1UNPS4thfuylzyYMRdW1duADwBXdVMDmh7fAl4HnA8cAr4y0WpWIMlLgZuBa6vqn5OuZxgL9GUmx6Wqnq6q8+l9m/+CJG8a9jFbCvemLnlQVY93t0eAn9Cbdpp1h7u50uNzpkcmXM9Jq6rD3RPyGeDbzMj4dHO6NwM/rKpbuuaZHJeF+jKr43JcVf0duAvYxJDj0lK4N3PJgySnd28SkeR04P3Ag0vfaybsBrZ2y1uBWydYy1COP+k6H2YGxqd74+67wL6q+mrfppkbl8X6MqPjsjrJK7vllwDvBf7AkOPSzKdlALqPPX2N/13yYPtkKzo5SV5L72wdepeI+NGs9SXJj4F307ts6WHg88BPgV3Aq4FHgcuqaurfqFykL++m96d/AQeATx+fH51WSd4J/Bp4AHima76e3lz1TI3LEn25nNkbl7fQe8N0Fb0T7l1V9cUkr2KIcWkq3CVJPS1Ny0iSOoa7JDXIcJekBo3rZ/ZW5Kyzzqq5ublJl7FiD/z1H0tuf/PaVwy030r2Pb7foEZV40qPK2n89uzZ88Tz/RuqKzI3N8f8/MxdG2vZX3Of737JfZBffR903/kV/jr8qGpc6XEljV+Svyy2zWkZSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBk3FVSGnzXJXSDzgFRIlTTnP3CWpQQOHe5JVSX6b5Gfd+plJbk/ySHd7Rt++1yXZn+ThJBeNo3BJ0uJWcuZ+DbCvb30bcEdVbQDu6NZJch6wBXgjsAn4ZpJVoylXkjSIgcI9yTrgEuA7fc2bgZ3d8k7g0r72m6rq31X1Z2A/cMFIqpUkDWTQM/evAZ8FnulrO6eqDgF0t2d37WuBx/r2O9i1PUuSK5PMJ5k/evToSuuWJC1h2XBP8kHgSFXtGfAxs0BbPaeh6saq2lhVG1evXvD3XSVJJ2mQj0JeCHwoycXAi4GXJ/kBcDjJmqo6lGQNcKTb/yCwvu/+64DHR1m0JGlpy565V9V1VbWuqubovVH6y6r6OLAb2NrtthW4tVveDWxJ8qIk5wIbgHtHXrkkaVHDfIlpB7ArySeBR4HLAKpqb5JdwEPAU8BVVfX00JVKkga2onCvqruAu7rlvwHvWWS/7cD2IWuTJJ0kv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGrRsuCdZn+TOJPuS7E1yTdd+ZpLbkzzS3Z7Rd5/rkuxP8nCSi8bZAUnScw1y5v4U8JmqegPwDuCqJOcB24A7qmoDcEe3TrdtC/BGYBPwzSSrxlG8JGlhy4Z7VR2qqvu65X8B+4C1wGZgZ7fbTuDSbnkzcFNV/buq/gzsBy4Ycd2SpCWsaM49yRzwVuAe4JyqOgS9FwDg7G63tcBjfXc72LWd+FhXJplPMn/06NGTKF2StJiBwz3JS4GbgWur6p9L7bpAWz2noerGqtpYVRtXr149aBmSpAEMFO5JTqMX7D+sqlu65sNJ1nTb1wBHuvaDwPq+u68DHh9NuZKkQQzyaZkA3wX2VdVX+zbtBrZ2y1uBW/vatyR5UZJzgQ3AvaMrWZK0nBcMsM+FwBXAA0nu79quB3YAu5J8EngUuAygqvYm2QU8RO+TNldV1dOjLlyStLhlw72q7mbheXSA9yxyn+3A9iHqkiQNwW+oSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWiQS/5KJ2Vu221Lbj+w45LnqRLp1OOZuyQ1yHCXpAYZ7pLUIOfcZ5Tz2ZKW4pm7JDXolDpz92xX0qnCM3dJapDhLkkNOqWmZaST5ZSeZo3hLjXCFyD1c1pGkhpkuEtSgwx3SWqQc+7SBDg/Pp1aGhfDXc1p6QnakuXGBRybUXJaRpIa5Jm7VsSzL2llJvWX5NjCPckm4OvAKuA7VbVjXMfyz3CdrGn/f8cXU52ssYR7klXAN4D3AQeB3yTZXVUPjeN4kgY36he0aX+BHJdp7/e4ztwvAPZX1Z8AktwEbAYMd2lGTHt4weA1zkJfRi1VNfoHTT4KbKqqT3XrVwBvr6qr+/a5EriyW3098PAISzgLeGKEjzdJ9mU62ZfpdKr15TVVtXqhDeM6c88Cbc96FamqG4Ebx3LwZL6qNo7jsZ9v9mU62ZfpZF/+Z1wfhTwIrO9bXwc8PqZjSZJOMK5w/w2wIcm5SV4IbAF2j+lYkqQTjGVapqqeSnI18HN6H4X8XlXtHcexFjGW6Z4JsS/Tyb5MJ/vSGcsbqpKkyfLyA5LUIMNdkhrUVLgn2ZTk4ST7k2ybdD3DSHIgyQNJ7k8yP+l6VirJ95IcSfJgX9uZSW5P8kh3e8YkaxzUIn25Iclfu/G5P8nFk6xxEEnWJ7kzyb4ke5Nc07XP3Lgs0ZdZHJcXJ7k3ye+6vnyhax9qXJqZc+8uefBH+i55AFw+q5c8SHIA2FhVM/mFjCTvAo4B36+qN3VtXwKerKod3YvvGVX1uUnWOYhF+nIDcKyqvjzJ2lYiyRpgTVXdl+RlwB7gUuATzNi4LNGXjzF74xLg9Ko6luQ04G7gGuAjDDEuLZ25//eSB1X1H+D4JQ80AVX1K+DJE5o3Azu75Z30noxTb5G+zJyqOlRV93XL/wL2AWuZwXFZoi8zp3qOdaundf8VQ45LS+G+Fnisb/0gMzrYnQJ+kWRPd6mGFpxTVYeg9+QEzp5wPcO6Osnvu2mbqZ/K6JdkDngrcA8zPi4n9AVmcFySrEpyP3AEuL2qhh6XlsJ92UsezJgLq+ptwAeAq7qpAU2PbwGvA84HDgFfmWg1K5DkpcDNwLVV9c9J1zOMBfoyk+NSVU9X1fn0vs1/QZI3DfuYLYV7U5c8qKrHu9sjwE/oTTvNusPdXOnxOdMjE67npFXV4e4J+QzwbWZkfLo53ZuBH1bVLV3zTI7LQn2Z1XE5rqr+DtwFbGLIcWkp3Ju55EGS07s3iUhyOvB+4MGl7zUTdgNbu+WtwK0TrGUox590nQ8zA+PTvXH3XWBfVX21b9PMjctifZnRcVmd5JXd8kuA9wJ/YMhxaebTMgDdx56+xv8uebB9shWdnCSvpXe2Dr1LRPxo1vqS5MfAu+ldtvQw8Hngp8Au4NXAo8BlVTX1b1Qu0pd30/vTv4ADwKePz49OqyTvBH4NPAA80zVfT2+ueqbGZYm+XM7sjctb6L1huoreCfeuqvpiklcxxLg0Fe6SpJ6WpmUkSR3DXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXo/wEHPWzZCblGjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].bar(np.arange(nvocab), wpht_h)\n",
    "ax[0].set_ylim(0, 500)\n",
    "\n",
    "ax[1].bar(np.arange(nvocab), wpht_m)\n",
    "ax[1].set_ylim(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7abe8547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'upon'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federalist_vocabulary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3eb7c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff615e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Business Data Science. Matt Taddy. (Chapter 8).\n",
    "* Inference and Disputed Authorship: The Federalist. Mosteller, Frederick and Wallace, David L. https://www.jstor.org/stable/202633\n",
    "* Updating: A Set of Bayesian Notes. Jeffrey B. Arnold. (Chapter 4). https://jrnold.github.io/bayesian_notes/index.html\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
